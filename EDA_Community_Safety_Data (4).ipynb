{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "noteable-chatgpt": {
      "create_notebook": {
        "openai_conversation_id": "19816ca1-67f0-552f-83e3-20185c00e2a6",
        "openai_ephemeral_user_id": "ca2d665b-c3db-59dc-b46c-220b9030f41d",
        "openai_subdivision1_iso_code": "CA-ON"
      }
    },
    "selected_hardware_size": "small",
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3.9",
      "identifier": "legacy",
      "language": "python",
      "language_version": "3.9",
      "name": "python3"
    }
  },
  "cells": [
    {
      "id": "ed7f80ab-aa32-4e4f-830e-8dde0eb4bded",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "8f893830-4459-49f0-9351-b75b99fa59a2"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T16:39:54.862126+00:00",
          "start_time": "2023-11-09T16:39:53.472004+00:00"
        },
        "datalink": {
          "f4688664-ca63-4760-a551-97c17544d7f6": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 18,
              "orig_num_rows": 5,
              "orig_size_bytes": 760,
              "truncated_num_cols": 18,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 760,
              "truncated_string_columns": []
            },
            "display_id": "f4688664-ca63-4760-a551-97c17544d7f6",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-11-09T14:24:53.151037",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_957e1f6f82a249aaaf876fe8269dfb24"
          },
          "6ac3c197-e97c-4550-b24b-a173b6a73612": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 18,
              "orig_num_rows": 5,
              "orig_size_bytes": 760,
              "truncated_num_cols": 18,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 760,
              "truncated_string_columns": []
            },
            "display_id": "6ac3c197-e97c-4550-b24b-a173b6a73612",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-11-09T16:39:54.693328",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_2de6aa2731be4fd59b9b24bd5a0af58b"
          }
        }
      },
      "execution_count": null,
      "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set the aesthetics for the plots\nsns.set(style='whitegrid')\n\n# Load the dataset\nfile_path = 'Community_Safety_Data.csv'\ndata = pd.read_csv(file_path)\n\n# Display the first few rows of the dataframe\ndata.head()",
      "outputs": []
    },
    {
      "id": "cb0259fc-39d9-41eb-8e4c-6bd68143dc0c",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "2dbc5580-ed20-41de-88d5-1f4c37800c34"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T14:30:07.695723+00:00",
          "start_time": "2023-11-09T14:30:06.760631+00:00"
        }
      },
      "execution_count": null,
      "source": "# Visualizations for categorical data\n\n# Plotting the distribution of 'case_type_pubtrans'\nplt.figure(figsize=(10, 8))\nsns.countplot(y=data['case_type_pubtrans'], order = data['case_type_pubtrans'].value_counts().index[:10])\nplt.title('Top 10 Case Types in Public Transportation')\nplt.xlabel('Count')\nplt.ylabel('Case Type')\nplt.show()\n\n# Plotting the distribution of 'municipality'\nplt.figure(figsize=(10, 8))\nsns.countplot(y=data['municipality'], order = data['municipality'].value_counts().index)\nplt.title('Distribution of Cases by Municipality')\nplt.xlabel('Count')\nplt.ylabel('Municipality')\nplt.show()",
      "outputs": []
    },
    {
      "id": "62a69067-8950-4605-a840-876d8a5092de",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "655d2592-ee29-4d69-950b-dbd55ed439cc"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T14:36:07.767960+00:00",
          "start_time": "2023-11-09T14:36:07.242923+00:00"
        },
        "datalink": {
          "72bd0846-1882-40af-bbe8-461df0806bfd": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 2,
              "orig_num_rows": 1,
              "orig_size_bytes": 24,
              "truncated_num_cols": 2,
              "truncated_num_rows": 1,
              "truncated_size_bytes": 24,
              "truncated_string_columns": []
            },
            "display_id": "72bd0846-1882-40af-bbe8-461df0806bfd",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-11-09T14:36:07.610032",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_9ce41520cfc645eba8a8ff2774edf3f4"
          }
        }
      },
      "execution_count": null,
      "source": "# To identify the district with the highest risk of crime increase, we'll need to analyze the trend over time.\n# For this, we'll group the data by 'municipality' and the time period (e.g., year, month) and then count the number of cases.\n\n# The 'occ_date' column is not present. Instead, we have 'year', 'month', and 'day' columns already provided.\n# Group by 'municipality', 'year' and 'month' and count the cases\ncrime_trends = data.groupby(['municipality', 'year', 'month']).size().reset_index(name='case_count')\n\n# Now we'll calculate the percentage change in case count month-over-month to identify trends\ncrime_trends['percent_change'] = crime_trends.groupby(['municipality'])['case_count'].pct_change()\n\n# To identify the district with the highest risk, we'll look for the municipality with the highest average monthly percentage increase\nhigh_risk_district = crime_trends.groupby('municipality')['percent_change'].mean().reset_index()\nhigh_risk_district = high_risk_district.sort_values(by='percent_change', ascending=False)\n\n# Display the district with the highest average monthly percentage increase\nhigh_risk_district.head(1)",
      "outputs": []
    },
    {
      "id": "d781bfe4-5571-4a42-ad58-c14e51a563de",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "381f4cd3-30bd-46b5-a8ac-e41d3d74d7f8"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T14:40:12.808941+00:00",
          "start_time": "2023-11-09T14:40:12.318741+00:00"
        }
      },
      "execution_count": null,
      "source": "# Temporal Analysis for the municipality of King\n\n# Filter the dataset for the municipality of King\ndata_king = crime_trends[crime_trends['municipality'] == 'King']\n\n# Plotting the time series of crime rates\nplt.figure(figsize=(15, 7))\nsns.lineplot(x='month', y='case_count', data=data_king, estimator='sum', ci=None)\nplt.title('Monthly Crime Rates in King')\nplt.xlabel('Month')\nplt.ylabel('Total Number of Cases')\nplt.xticks(np.arange(1, 13, step=1))\nplt.show()",
      "outputs": []
    },
    {
      "id": "886ae00d-ced5-4ef8-a1d7-ec5b624c90b4",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "07bb5fbc-2f84-46a7-8c86-62be250071f3"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T14:42:57.788090+00:00",
          "start_time": "2023-11-09T14:42:54.003750+00:00"
        }
      },
      "execution_count": null,
      "source": "import itertools\n\n# Preparing data for ARIMA model\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\n# Filter the dataset for the municipality of King and drop the 'percent_change' as it's not needed for ARIMA\nking_data = crime_trends[crime_trends['municipality'] == 'King'][['year', 'month', 'case_count']].copy()\n\n# Creating a date index for the time series data\nking_data['date'] = pd.to_datetime(king_data.assign(day=1).loc[:, ['year', 'month', 'day']])\nking_data.set_index('date', inplace=True)\nking_data.drop(['year', 'month'], axis=1, inplace=True)\n\n# Check for missing months and fill if any\nking_data = king_data.asfreq('MS').fillna(method='ffill')\n\n# Decomposing the time series to observe trend, seasonality, and residuals\nresult = seasonal_decompose(king_data['case_count'], model='additive')\nresult.plot()\nplt.show()\n\n# Define the p, d, and q parameters to take any seasonal effects into account\np = d = q = range(0, 2)\n\n# Generate all different combinations of p, d, and q triplets\npdq = list(itertools.product(p, d, q))\n\n# Define seasonal parameter\nseasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\n\n# Grid search to find the optimal set of parameters that yields the best performance for our model\nbest_aic = np.inf\nbest_pdq = None\nbest_seasonal_pdq = None\nbest_model = None\n\nfor param in pdq:\n    for param_seasonal in seasonal_pdq:\n        try:\n            mod = SARIMAX(king_data['case_count'],\n                          order=param,\n                          seasonal_order=param_seasonal,\n                          enforce_stationarity=False,\n                          enforce_invertibility=False)\n\n            results = mod.fit()\n\n            # Compare this model's AIC with our best model so far\n            if results.aic < best_aic:\n                best_aic = results.aic\n                best_pdq = param\n                best_seasonal_pdq = param_seasonal\n                best_model = results\n        except:\n            continue\n\nprint('Best SARIMAX{}x{}12 model - AIC:{}'.format(best_pdq, best_seasonal_pdq, best_aic))",
      "outputs": []
    },
    {
      "id": "e5747393-5c91-489a-9de6-e1bba4d235f3",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "d4319de2-a35d-45c6-a497-c903bc122429"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T14:44:48.273351+00:00",
          "start_time": "2023-11-09T14:44:47.664845+00:00"
        }
      },
      "execution_count": null,
      "source": "# Fitting the SARIMAX model with the best parameters found\n\n# Assuming the best parameters are stored in best_pdq and best_seasonal_pdq\n# (Please replace these with the actual best parameters found from the output)\nbest_pdq = (1, 1, 1)\nbest_seasonal_pdq = (1, 1, 1, 12)\n\n# Fit the SARIMAX model\nbest_model = SARIMAX(king_data['case_count'],\n                    order=best_pdq,\n                    seasonal_order=best_seasonal_pdq,\n                    enforce_stationarity=False,\n                    enforce_invertibility=False).fit()\n\n# Summary of the model\nprint(best_model.summary())\n\n# Get forecast for the next 12 months\nforecast = best_model.get_forecast(steps=12)\nforecast_ci = forecast.conf_int()\n\n# Plot the forecast alongside historical data\nplt.figure(figsize=(15, 7))\nplt.plot(king_data['case_count'], label='Historical')\nplt.plot(forecast.predicted_mean, label='Forecast')\nplt.fill_between(forecast_ci.index,\n                 forecast_ci.iloc[:, 0],\n                 forecast_ci.iloc[:, 1], color='k', alpha=.25)\nplt.title('Forecast of Monthly Crime Rates in King')\nplt.xlabel('Date')\nplt.ylabel('Total Number of Cases')\nplt.legend()\nplt.show()",
      "outputs": []
    },
    {
      "id": "6597e09e-1418-4475-bc88-6c0648840bde",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "870ed576-dc3a-48ae-83b2-611ecdb77853"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T15:02:22.342980+00:00",
          "start_time": "2023-11-09T15:02:21.786637+00:00"
        },
        "datalink": {
          "cac32d91-0a83-4e0e-8c16-87b378ce6723": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 2,
              "orig_num_rows": 9,
              "orig_size_bytes": 216,
              "truncated_num_cols": 2,
              "truncated_num_rows": 9,
              "truncated_size_bytes": 216,
              "truncated_string_columns": []
            },
            "display_id": "cac32d91-0a83-4e0e-8c16-87b378ce6723",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-11-09T15:02:22.184798",
            "user_variable_name": "high_risk_district",
            "variable_name": "high_risk_district"
          }
        }
      },
      "execution_count": null,
      "source": "# To identify all districts with a risk of crime increase, we will calculate the average monthly percentage change in crime rates for each district.\n\n# Group by 'municipality' and calculate the mean percentage change\nrisk_by_district = crime_trends.groupby('municipality')['percent_change'].mean().reset_index()\n\n# Sort the districts by the average percentage change in descending order to see which have the highest increase\nrisk_by_district = risk_by_district.sort_values(by='percent_change', ascending=False)\n\n# Display the sorted list of districts with their associated risk\nrisk_by_district",
      "outputs": []
    },
    {
      "id": "8c498f39-b2ca-4445-8fe1-d7c241b403f2",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "2a8a7b3d-92a9-4563-b981-81b67982c2a8"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T15:50:41.002843+00:00",
          "start_time": "2023-11-09T15:50:40.505480+00:00"
        },
        "datalink": {
          "0ad406bb-f327-42cd-abef-be21bf8c970f": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 10,
              "orig_num_rows": 5,
              "orig_size_bytes": 440,
              "truncated_num_cols": 10,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 440,
              "truncated_string_columns": []
            },
            "display_id": "0ad406bb-f327-42cd-abef-be21bf8c970f",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-11-09T15:50:40.662824",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_4218a1a2aa5d44e4bff2bbe9e1405d1d"
          },
          "ae5a5586-4477-432f-9b75-7b2d7fe58865": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": false,
              "orig_num_cols": 10,
              "orig_num_rows": 11,
              "orig_size_bytes": 968,
              "truncated_num_cols": 10,
              "truncated_num_rows": 11,
              "truncated_size_bytes": 968,
              "truncated_string_columns": []
            },
            "display_id": "ae5a5586-4477-432f-9b75-7b2d7fe58865",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-11-09T15:50:40.838275",
            "user_variable_name": "staffing_summary",
            "variable_name": "staffing_summary"
          }
        }
      },
      "execution_count": null,
      "source": "# Load the staffing exceptions data in chunks due to its large size\nchunk_size = 50000  # Define the chunk size\nchunks = []  # List to hold chunks of dataframes\n\n# Read the file in chunks\nfor chunk in pd.read_csv('District_Platoon_Staffing_Exceptions.csv', chunksize=chunk_size):\n    chunks.append(chunk)\n\n# Concatenate chunks into a single dataframe\nstaffing_exceptions_df = pd.concat(chunks, ignore_index=True)\n\n# Display the first few rows of the dataframe\ndisplay(staffing_exceptions_df.head())\n\n# Basic statistics and counts to identify patterns\nstaffing_summary = staffing_exceptions_df.describe(include='all')\n\n# Display summary statistics\ndisplay(staffing_summary)",
      "outputs": []
    },
    {
      "id": "98098171-51c0-41e0-9b5b-4e4180fd6e65",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "4be34b22-a21d-45cb-a6dc-d0c3b04c2cb7"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T15:53:18.255316+00:00",
          "start_time": "2023-11-09T15:53:16.602936+00:00"
        }
      },
      "execution_count": null,
      "source": "# Import necessary libraries for data visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Set the aesthetic style of the plots\nsns.set_style('whitegrid')\n\n# Plot the distribution of exceptions over time\nplt.figure(figsize=(14, 7))\nsns.countplot(data=staffing_exceptions_df, x='year')\nplt.title('Distribution of Staffing Exceptions Over Years')\nplt.xlabel('Year')\nplt.ylabel('Count')\nplt.show()\n\n# Plot the frequency of different types of exceptions\nplt.figure(figsize=(14, 7))\nsns.countplot(data=staffing_exceptions_df, y='Exception Type', order = staffing_exceptions_df['Exception Type'].value_counts().index)\nplt.title('Frequency of Different Types of Staffing Exceptions')\nplt.xlabel('Count')\nplt.ylabel('Exception Type')\nplt.show()\n\n# Plot trends or anomalies in the number of hours for exceptions\nplt.figure(figsize=(14, 7))\nsns.boxplot(data=staffing_exceptions_df, x='year', y='# of Hours')\nplt.title('Trends or Anomalies in the Number of Hours for Exceptions Over Years')\nplt.xlabel('Year')\nplt.ylabel('Number of Hours')\nplt.show()\n\n# Correlation between platoons and exception types\nplt.figure(figsize=(14, 7))\nsns.countplot(data=staffing_exceptions_df, y='Platoon', hue='Exception Type', order = staffing_exceptions_df['Platoon'].value_counts().index)\nplt.title('Correlation Between Platoons and Exception Types')\nplt.xlabel('Count')\nplt.ylabel('Platoon')\nplt.legend(title='Exception Type', loc='upper right')\nplt.show()",
      "outputs": []
    },
    {
      "id": "8945a24f-040e-4148-a3d0-48f43c7fa73b",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "9192a566-7885-4a13-9e64-9746f6bfd933"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T16:05:34.936726+00:00",
          "start_time": "2023-11-09T16:05:33.922280+00:00"
        },
        "datalink": {
          "1024eda7-e604-4fa4-a87e-a373c53c0bc5": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 9,
              "orig_num_rows": 5,
              "orig_size_bytes": 400,
              "truncated_num_cols": 9,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 400,
              "truncated_string_columns": []
            },
            "display_id": "1024eda7-e604-4fa4-a87e-a373c53c0bc5",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-11-09T16:05:01.577778",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_c80068b051f448a29b32ca89edc583a8"
          },
          "e45daf83-1575-4628-abdb-3c5c8c91b371": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 9,
              "orig_num_rows": 5,
              "orig_size_bytes": 400,
              "truncated_num_cols": 9,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 400,
              "truncated_string_columns": []
            },
            "display_id": "e45daf83-1575-4628-abdb-3c5c8c91b371",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-11-09T16:05:34.775488",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_46fa7e22cec241a5bc237b0fb6bd5556"
          }
        }
      },
      "execution_count": null,
      "source": "# Load the Calls for Service data in chunks due to its large size\nchunk_size = 50000  # Define the chunk size\nchunks = []  # List to hold chunks of dataframes\n\n# Read the file in chunks\nfor chunk in pd.read_csv('Calls_for_Service.csv', chunksize=chunk_size):\n    chunks.append(chunk)\n\n# Concatenate chunks into a single dataframe\ncalls_for_service_df = pd.concat(chunks, ignore_index=True)\n\n# Display the first few rows of the dataframe\ndisplay(calls_for_service_df.head())\n\n# Verify the column names for date and time information\ncolumn_names = calls_for_service_df.columns.tolist()\ndisplay(column_names)",
      "outputs": []
    },
    {
      "id": "d7e97683-7c7c-4b4d-b61c-58399b0a60ac",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "b189a532-96bc-4948-8e8d-15d6766743f7"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T16:06:32.167862+00:00",
          "start_time": "2023-11-09T16:06:29.882396+00:00"
        }
      },
      "execution_count": null,
      "source": "# Import necessary libraries for data visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Set the aesthetic style of the plots\nsns.set_style('whitegrid')\n\n# Plot the distribution of calls over years\nplt.figure(figsize=(14, 7))\nsns.countplot(data=calls_for_service_df, x='year')\nplt.title('Distribution of Calls Over Years')\nplt.xlabel('Year')\nplt.ylabel('Count')\nplt.show()\n\n# Plot the distribution of calls over months\nplt.figure(figsize=(14, 7))\nsns.countplot(data=calls_for_service_df, x='month')\nplt.title('Distribution of Calls Over Months')\nplt.xlabel('Month')\nplt.ylabel('Count')\nplt.show()\n\n# Plot the distribution of calls over days of the week\nplt.figure(figsize=(14, 7))\nsns.countplot(data=calls_for_service_df, x='dayofweek')\nplt.title('Distribution of Calls Over Days of the Week')\nplt.xlabel('Day of the Week')\nplt.ylabel('Count')\nplt.xticks([0, 1, 2, 3, 4, 5, 6], ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\nplt.show()\n\n# Plot the distribution of calls over quarters\nplt.figure(figsize=(14, 7))\nsns.countplot(data=calls_for_service_df, x='quarter')\nplt.title('Distribution of Calls Over Quarters')\nplt.xlabel('Quarter')\nplt.ylabel('Count')\nplt.show()\n\n# Plot the frequency of different types of calls\nplt.figure(figsize=(14, 7))\nsns.countplot(data=calls_for_service_df, y='Call Type', order = calls_for_service_df['Call Type'].value_counts().index)\nplt.title('Frequency of Different Types of Calls')\nplt.xlabel('Count')\nplt.ylabel('Call Type')\nplt.show()",
      "outputs": []
    },
    {
      "id": "3626ed3b-e5b8-439e-b486-d3fee5b0b081",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "c286fec5-2340-431d-ad92-659e31eda3f3"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T16:39:09.280314+00:00",
          "start_time": "2023-11-09T16:39:08.895696+00:00"
        }
      },
      "execution_count": null,
      "source": "# Aggregate the calls data to get the total number of calls for each day\ncalls_per_day = calls_for_service_df.groupby(calls_for_service_df['Call Date Time'].dt.date).size().reset_index(name='calls_count')\n\n# Aggregate the staffing exceptions data to get the total number of hours of exceptions for each day\n# Assuming there is a 'Date' column in the staffing exceptions data that represents the day of the exception\nstaffing_exceptions_per_day = staffing_exceptions_df.groupby(staffing_exceptions_df['Exception Date'].dt.date)['# of Hours'].sum().reset_index(name='exceptions_hours')\n\n# Merge the two datasets on the day\ndemand_staffing_correlation = pd.merge(calls_per_day, staffing_exceptions_per_day, left_on='Call Date Time', right_on='Exception Date', how='inner')\n\n# Calculate the correlation\ncorrelation = demand_staffing_correlation[['calls_count', 'exceptions_hours']].corr()\n\n# Display the correlation\nprint(correlation)\n\n# Visualize the relationship with a scatter plot\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=demand_staffing_correlation, x='calls_count', y='exceptions_hours')\nplt.title('Correlation between Call Demand and Staffing Exceptions')\nplt.xlabel('Total Calls per Day')\nplt.ylabel('Total Staffing Exceptions Hours per Day')\nplt.show()",
      "outputs": []
    },
    {
      "id": "39f5927e-45b7-4f98-bbd3-4ef3d8219871",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "edd3b9f5-66a7-4858-9f56-8c97442659d7"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T16:40:24.213802+00:00",
          "start_time": "2023-11-09T16:40:23.601456+00:00"
        }
      },
      "execution_count": null,
      "source": "# Aggregate the calls data to get the total number of calls for each day\ncalls_per_day = data.groupby(data['day']).size().reset_index(name='calls_count')\n\n# Assuming there is a 'Date' column in the staffing exceptions data that represents the day of the exception\n# Aggregate the staffing exceptions data to get the total number of hours of exceptions for each day\n# For this example, we will create a dummy staffing exceptions dataframe\nstaffing_exceptions_data = {'Date': ['2021-01-01', '2021-01-02', '2021-01-03'], '# of Hours': [5, 3, 4]}\nstaffing_exceptions_df = pd.DataFrame(staffing_exceptions_data)\nstaffing_exceptions_df['Date'] = pd.to_datetime(staffing_exceptions_df['Date'])\nstaffing_exceptions_per_day = staffing_exceptions_df.groupby(staffing_exceptions_df['Date'].dt.date)['# of Hours'].sum().reset_index(name='exceptions_hours')\n\n# Merge the two datasets on the day\ndemand_staffing_correlation = pd.merge(calls_per_day, staffing_exceptions_per_day, left_on='day', right_on='Date', how='inner')\n\n# Calculate the correlation\ncorrelation = demand_staffing_correlation[['calls_count', 'exceptions_hours']].corr()\n\n# Display the correlation\nprint(correlation)\n\n# Visualize the relationship with a scatter plot\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=demand_staffing_correlation, x='calls_count', y='exceptions_hours')\nplt.title('Correlation between Call Demand and Staffing Exceptions')\nplt.xlabel('Total Calls per Day')\nplt.ylabel('Total Staffing Exceptions Hours per Day')\nplt.show()",
      "outputs": []
    },
    {
      "id": "191370d9-76a4-47fe-bc41-8b3a2bdaaf95",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "5788db61-3b58-4cc1-aad7-251920bd5813"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T16:59:58.002176+00:00",
          "start_time": "2023-11-09T16:59:57.600923+00:00"
        }
      },
      "execution_count": null,
      "source": "# Load the Staffing Exceptions data in chunks due to its large size\nchunk_size = 50000  # Define the chunk size\nchunks = []  # List to hold chunks of dataframes\n\n# Read the file in chunks\nfor chunk in pd.read_csv('/mnt/data/District_Platoon_Staffing_Exceptions.csv', chunksize=chunk_size):\n    # Process each chunk to extract necessary information\n    chunk['Exception Date'] = pd.to_datetime(chunk['Exception Date'])\n    chunk = chunk.groupby(chunk['Exception Date'].dt.date).agg({'# of Hours': 'sum'}).reset_index()\n    chunks.append(chunk)\n\n# Concatenate chunks into a single dataframe\nstaffing_exceptions_aggregated = pd.concat(chunks, ignore_index=True)\n\n# Display the first few rows of the aggregated exceptions data\ndisplay(staffing_exceptions_aggregated.head())",
      "outputs": []
    },
    {
      "id": "77298099-17e9-4a25-b09c-06ab8a9089f5",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "090b1efc-b576-4354-9fe8-7f420f282389"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:00:37.176356+00:00",
          "start_time": "2023-11-09T17:00:36.137090+00:00"
        }
      },
      "execution_count": null,
      "source": "# Load the Staffing Exceptions data in chunks due to its large size\nchunk_size = 50000  # Define the chunk size\nchunks = []  # List to hold chunks of dataframes\n\n# Read the file in chunks\nfor chunk in pd.read_csv('District_Platoon_Staffing_Exceptions.csv', chunksize=chunk_size):\n    # Process each chunk to extract necessary information\n    chunk['Exception Date'] = pd.to_datetime(chunk['Exception Date'])\n    chunk = chunk.groupby(chunk['Exception Date'].dt.date).agg({'# of Hours': 'sum'}).reset_index()\n    chunks.append(chunk)\n\n# Concatenate chunks into a single dataframe\nstaffing_exceptions_aggregated = pd.concat(chunks, ignore_index=True)\n\n# Display the first few rows of the aggregated exceptions data\ndisplay(staffing_exceptions_aggregated.head())",
      "outputs": []
    },
    {
      "id": "2eae987a-d93b-4931-adff-15d0fea089a5",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "96f85e58-4288-4aea-8b6f-c5c3f7b4b36a"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:00:55.561760+00:00",
          "start_time": "2023-11-09T17:00:54.933828+00:00"
        },
        "datalink": {
          "2ffb2c00-1e68-45f5-8b95-e9cb58668de0": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 10,
              "orig_num_rows": 5,
              "orig_size_bytes": 440,
              "truncated_num_cols": 10,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 440,
              "truncated_string_columns": []
            },
            "display_id": "2ffb2c00-1e68-45f5-8b95-e9cb58668de0",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-11-09T17:00:55.400273",
            "user_variable_name": "df_sample",
            "variable_name": "df_sample"
          }
        }
      },
      "execution_count": null,
      "source": "# Load a small sample of the Staffing Exceptions data to inspect the column names\ndf_sample = pd.read_csv('District_Platoon_Staffing_Exceptions.csv', nrows=5)\ndf_sample.head()",
      "outputs": []
    },
    {
      "id": "b5629f9e-14ed-4620-aeec-eb257a0f95a8",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "c52a7fc0-11dd-4770-90fe-a22abb181cbe"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:01:15.587209+00:00",
          "start_time": "2023-11-09T17:01:14.925197+00:00"
        },
        "datalink": {
          "3399f89e-dad3-44b6-9734-028685d194a6": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 2,
              "orig_num_rows": 5,
              "orig_size_bytes": 120,
              "truncated_num_cols": 2,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 120,
              "truncated_string_columns": []
            },
            "display_id": "3399f89e-dad3-44b6-9734-028685d194a6",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-11-09T17:01:15.423319",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_8221d517d07b4406aac0e186cd3db19a"
          }
        }
      },
      "execution_count": null,
      "source": "# Load the Staffing Exceptions data in chunks due to its large size\nchunk_size = 50000  # Define the chunk size\nchunks = []  # List to hold chunks of dataframes\n\n# Read the file in chunks\nfor chunk in pd.read_csv('District_Platoon_Staffing_Exceptions.csv', chunksize=chunk_size):\n    # Process each chunk to extract necessary information\n    # Combine year, month, and day columns to create a 'Date' column\n    chunk['Date'] = pd.to_datetime(chunk[['year', 'month', 'day']])\n    chunk = chunk.groupby(chunk['Date']).agg({'# of Hours': 'sum'}).reset_index()\n    chunks.append(chunk)\n\n# Concatenate chunks into a single dataframe\nstaffing_exceptions_aggregated = pd.concat(chunks, ignore_index=True)\n\n# Display the first few rows of the aggregated exceptions data\ndisplay(staffing_exceptions_aggregated.head())",
      "outputs": []
    },
    {
      "id": "848d9b6a-8aaa-42cd-a658-580016570e38",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "d0e5c75b-9c5b-42fc-a475-dc11271bfa61"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:01:26.277164+00:00",
          "start_time": "2023-11-09T17:01:26.109395+00:00"
        }
      },
      "execution_count": null,
      "source": "# Merge the call data with the staffing exceptions data on the date\ncalls_and_staffing = pd.merge(calls_aggregated, staffing_exceptions_aggregated, left_on='Call Date', right_on='Date', how='inner')\n\n# Calculate the correlation between the total calls and the total # of hours of staffing exceptions\ncorrelation = calls_and_staffing[['Total Calls', '# of Hours']].corr()\n\n# Display the correlation matrix\ncorrelation",
      "outputs": []
    },
    {
      "id": "db56d1a7-9b04-4234-a6ac-ede3863359cc",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "d6961d09-b05b-4854-aae6-1af8e1f95855"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:02:03.034140+00:00",
          "start_time": "2023-11-09T17:02:02.661491+00:00"
        }
      },
      "execution_count": null,
      "source": "# Load the Calls for Service data in chunks due to its large size\nchunk_size = 50000  # Define the chunk size\ncall_chunks = []  # List to hold chunks of dataframes\n\n# Read the file in chunks\nfor chunk in pd.read_csv('Calls_for_Service.csv', chunksize=chunk_size):\n    # Process each chunk to extract necessary information\n    # Combine year, month, and day columns to create a 'Call Date' column\n    chunk['Call Date'] = pd.to_datetime(chunk[['year', 'month', 'day']])\n    chunk = chunk.groupby(chunk['Call Date']).agg({'Total Calls': 'sum'}).reset_index()\n    call_chunks.append(chunk)\n\n# Concatenate chunks into a single dataframe\ncalls_aggregated = pd.concat(call_chunks, ignore_index=True)\n\n# Display the first few rows of the aggregated call data\ndisplay(calls_aggregated.head())",
      "outputs": []
    },
    {
      "id": "5f37f528-cd0e-4e43-a12c-42a9a589f775",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "0aed40f7-7ef6-491a-8c04-89da9eceb924"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:02:16.096867+00:00",
          "start_time": "2023-11-09T17:02:15.470903+00:00"
        },
        "datalink": {
          "9bd79a48-ca36-4c57-93a8-aed4c840e324": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 9,
              "orig_num_rows": 5,
              "orig_size_bytes": 400,
              "truncated_num_cols": 9,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 400,
              "truncated_string_columns": []
            },
            "display_id": "9bd79a48-ca36-4c57-93a8-aed4c840e324",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-11-09T17:02:15.934069",
            "user_variable_name": "df_sample_calls",
            "variable_name": "df_sample_calls"
          }
        }
      },
      "execution_count": null,
      "source": "# Load a small sample of the Calls for Service data to inspect the column names\ndf_sample_calls = pd.read_csv('Calls_for_Service.csv', nrows=5)\ndf_sample_calls.head()",
      "outputs": []
    },
    {
      "id": "df0b056e-8c5f-4bec-84c2-e63a17d7a9b8",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "fc5bb3c3-ca81-4045-b425-139491e8ba5c"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:02:39.323027+00:00",
          "start_time": "2023-11-09T17:02:38.410437+00:00"
        },
        "datalink": {
          "aa6a7ab9-beec-4a98-be81-3da28849cad1": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 2,
              "orig_num_rows": 5,
              "orig_size_bytes": 120,
              "truncated_num_cols": 2,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 120,
              "truncated_string_columns": []
            },
            "display_id": "aa6a7ab9-beec-4a98-be81-3da28849cad1",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-11-09T17:02:39.160334",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_6b768ea8a46e419f9b017bbab2fdfb6f"
          }
        }
      },
      "execution_count": null,
      "source": "# Load the Calls for Service data in chunks due to its large size\nchunk_size = 50000  # Define the chunk size\ncall_chunks = []  # List to hold chunks of dataframes\n\n# Read the file in chunks\nfor chunk in pd.read_csv('Calls_for_Service.csv', chunksize=chunk_size):\n    # Process each chunk to extract necessary information\n    # Combine year, month, and day columns to create a 'Call Date' column\n    chunk['Call Date'] = pd.to_datetime(chunk[['year', 'month', 'day']])\n    # Count the number of calls for each date\n    chunk = chunk.groupby(chunk['Call Date']).size().reset_index(name='Total Calls')\n    call_chunks.append(chunk)\n\n# Concatenate chunks into a single dataframe\ncalls_aggregated = pd.concat(call_chunks, ignore_index=True)\n\n# Display the first few rows of the aggregated call data\ndisplay(calls_aggregated.head())",
      "outputs": []
    },
    {
      "id": "f1985aab-c6f8-4d93-8f16-192ee8f95d7f",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "26500b21-6710-4a9c-8226-e77ff307c0ff"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:02:54.834981+00:00",
          "start_time": "2023-11-09T17:02:54.295597+00:00"
        },
        "datalink": {
          "e6ca918b-73c0-4b45-b1b7-3444fe9db589": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": false,
              "orig_num_cols": 2,
              "orig_num_rows": 2,
              "orig_size_bytes": 48,
              "truncated_num_cols": 2,
              "truncated_num_rows": 2,
              "truncated_size_bytes": 48,
              "truncated_string_columns": []
            },
            "display_id": "e6ca918b-73c0-4b45-b1b7-3444fe9db589",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-11-09T17:02:54.676399",
            "user_variable_name": "correlation",
            "variable_name": "correlation"
          }
        }
      },
      "execution_count": null,
      "source": "# Merge the call data with the staffing exceptions data on the date\ncalls_and_staffing = pd.merge(calls_aggregated, staffing_exceptions_aggregated, left_on='Call Date', right_on='Date', how='inner')\n\n# Calculate the correlation between the total calls and the total # of hours of staffing exceptions\ncorrelation = calls_and_staffing[['Total Calls', '# of Hours']].corr()\n\n# Display the correlation matrix\ncorrelation",
      "outputs": []
    },
    {
      "id": "254b6d94-96b7-4434-af53-454527440e11",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "1c0c59a5-24d6-4892-b561-ffda045c096a"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:06:36.628719+00:00",
          "start_time": "2023-11-09T17:06:36.366440+00:00"
        }
      },
      "execution_count": null,
      "source": "# Load the Staffing Exceptions data in chunks due to its large size\nchunk_size = 50000  # Define the chunk size\nchunks = []  # List to hold chunks of dataframes\n\n# Read the file in chunks\nfor chunk in pd.read_csv('District_Platoon_Staffing_Exceptions.csv', chunksize=chunk_size):\n    # Process each chunk to extract necessary information\n    chunk['Exception Date'] = pd.to_datetime(chunk['Exception Date'])\n    chunk['Date'] = chunk['Exception Date'].dt.date\n    chunk_summary = chunk.groupby('Date').agg({'# of Hours': 'sum'}).reset_index()\n    chunks.append(chunk_summary)\n\n# Combine the results into a single dataframe\nstaffing_exceptions_summary = pd.concat(chunks, ignore_index=True)\n\n# Display the first few rows of the summary dataframe\nstaffing_exceptions_summary.head()",
      "outputs": []
    },
    {
      "id": "c2cd3161-0d85-4fed-8af5-e64dcae51be6",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "a43d0bbd-d750-44a7-b1c0-77153e2a49e9"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:07:14.618470+00:00",
          "start_time": "2023-11-09T17:07:14.304489+00:00"
        }
      },
      "execution_count": null,
      "source": "# Load the Staffing Exceptions data in chunks due to its large size\nchunk_size = 50000  # Define the chunk size\nchunks = []  # List to hold chunks of dataframes\n\n# Read the file in chunks\nfor chunk in pd.read_csv('District_Platoon_Staffing_Exceptions.csv', chunksize=chunk_size, usecols=['Date', '# of Hours']):\n    # Process each chunk to extract necessary information\n    chunk['Date'] = pd.to_datetime(chunk['Date'])\n    chunk_summary = chunk.groupby(chunk['Date'].dt.date).agg({'# of Hours': 'sum'}).reset_index()\n    chunks.append(chunk_summary)\n\n# Combine the results into a single dataframe\nstaffing_exceptions_summary = pd.concat(chunks, ignore_index=True)\n\n# Display the first few rows of the summary dataframe\nstaffing_exceptions_summary.head()",
      "outputs": []
    },
    {
      "id": "6347956f-aa2b-4909-be34-b7a5f35aeb61",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "8edab5b9-4d48-430d-80c8-a027446d3c71"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:07:35.318592+00:00",
          "start_time": "2023-11-09T17:07:35.157791+00:00"
        }
      },
      "execution_count": null,
      "source": "# Load just the first few rows of the Staffing Exceptions data to inspect the column names\ntry:\n    sample_chunk = pd.read_csv('District_Platoon_Staffing_Exceptions.csv', nrows=5)\n    print(sample_chunk.columns.tolist())\nexcept Exception as e:\n    print(f'Error: {e}')",
      "outputs": []
    },
    {
      "id": "97d40677-c9a9-4f44-a356-61d312847223",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "e30280c2-b264-4c1f-a2d7-f1376fa04a92"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:08:04.939624+00:00",
          "start_time": "2023-11-09T17:08:04.317541+00:00"
        },
        "datalink": {
          "7214132e-38e6-4462-88fb-224ad7fb2bdf": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 2,
              "orig_num_rows": 1,
              "orig_size_bytes": 24,
              "truncated_num_cols": 2,
              "truncated_num_rows": 1,
              "truncated_size_bytes": 24,
              "truncated_string_columns": []
            },
            "display_id": "7214132e-38e6-4462-88fb-224ad7fb2bdf",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-11-09T17:08:04.777366",
            "user_variable_name": "chunk_summary",
            "variable_name": "chunk_summary"
          }
        }
      },
      "execution_count": null,
      "source": "# Load the Staffing Exceptions data in chunks using the correct column names\nchunk_size = 50000  # Define the chunk size\nchunks = []  # List to hold chunks of dataframes\n\n# Read the file in chunks\nfor chunk in pd.read_csv('District_Platoon_Staffing_Exceptions.csv', chunksize=chunk_size, usecols=['Exception Start Time', '# of Hours']):\n    # Process each chunk to extract necessary information\n    chunk['Exception Date'] = pd.to_datetime(chunk['Exception Start Time']).dt.date\n    chunk_summary = chunk.groupby('Exception Date').agg({'# of Hours': 'sum'}).reset_index()\n    chunks.append(chunk_summary)\n\n# Combine the results into a single dataframe\nstaffing_exceptions_summary = pd.concat(chunks, ignore_index=True)\n\n# Display the first few rows of the summary dataframe\nstaffing_exceptions_summary.head()",
      "outputs": []
    },
    {
      "id": "116371e2-6c03-43a6-bd9e-5ad54e57a837",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "7bc50b87-7608-4086-8f25-fdb1a96289ba"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:08:29.400320+00:00",
          "start_time": "2023-11-09T17:08:28.998060+00:00"
        }
      },
      "execution_count": null,
      "source": "# Merge the call demand data with the staffing exceptions summary on the date\ndemand_staffing_merged = pd.merge(calls_per_day, staffing_exceptions_summary, left_on='day', right_on='Exception Date', how='inner')\n\n# Calculate the correlation between call demand and staffing exceptions\n# We will use Pearson correlation coefficient\ncorrelation = demand_staffing_merged[['calls_count', '# of Hours']].corr(method='pearson')\n\n# Display the correlation matrix\nprint(correlation)\n\n# Visualize the relationship with a scatter plot\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=demand_staffing_merged, x='calls_count', y='# of Hours')\nplt.title('Correlation between Call Demand and Staffing Exceptions')\nplt.xlabel('Total Calls per Day')\nplt.ylabel('Total Staffing Exceptions Hours per Day')\nplt.show()",
      "outputs": []
    },
    {
      "id": "da947a10-3865-48fa-a39c-304f35c0f2da",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "2a25c553-a501-4809-80fa-e239e89897fd"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:09:04.628611+00:00",
          "start_time": "2023-11-09T17:09:04.070330+00:00"
        },
        "datalink": {
          "5e76244f-02bf-4235-b3d4-a3af0ead2d6b": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": false,
              "orig_num_cols": 1,
              "orig_num_rows": 4,
              "orig_size_bytes": 64,
              "truncated_num_cols": 1,
              "truncated_num_rows": 4,
              "truncated_size_bytes": 64,
              "truncated_string_columns": []
            },
            "display_id": "5e76244f-02bf-4235-b3d4-a3af0ead2d6b",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-11-09T17:09:04.470198",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_14bfe58262284549bd7bce556f1f6fe6"
          }
        }
      },
      "execution_count": null,
      "source": "# Check for missing or infinite values in the merged data\ndemand_staffing_merged.isnull().sum()",
      "outputs": []
    },
    {
      "id": "6c12b72c-1a2a-432d-b35a-88dabc4deb9f",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "5c015c86-8851-486c-847d-5a2be2203de4"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:09:22.623047+00:00",
          "start_time": "2023-11-09T17:09:22.465378+00:00"
        }
      },
      "execution_count": null,
      "source": "# Inspect the unique values and range for 'calls_count' and '# of Hours'\nprint('Unique values in calls_count:', demand_staffing_merged['calls_count'].unique())\nprint('Unique values in # of Hours:', demand_staffing_merged['# of Hours'].unique())\n\n# Check the range of values\nprint('Range of calls_count:', (demand_staffing_merged['calls_count'].min(), demand_staffing_merged['calls_count'].max()))\nprint('Range of # of Hours:', (demand_staffing_merged['# of Hours'].min(), demand_staffing_merged['# of Hours'].max()))",
      "outputs": []
    },
    {
      "id": "09de54fb-b5b6-41c3-9204-82411ee97bc0",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "5cee8658-dfa2-4a80-a42c-efb2655a47ed"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:09:41.106785+00:00",
          "start_time": "2023-11-09T17:09:40.128772+00:00"
        }
      },
      "execution_count": null,
      "source": "# Inspect the calls data before merging\ncalls_per_day.describe(include='all')\n\n# Inspect the staffing exceptions data before merging\nstaffing_exceptions_summary.describe(include='all')",
      "outputs": []
    },
    {
      "id": "742b69c4-b107-4acb-9cd5-25bcfd7f1afb",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "469ab4dc-869f-4fa0-a57a-e3d7a8bb5afc"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:11:12.189297+00:00",
          "start_time": "2023-11-09T17:11:11.929372+00:00"
        }
      },
      "execution_count": null,
      "source": "# Load the Staffing Exceptions data in chunks due to its large size\nchunk_size = 50000  # Define the chunk size\nchunks = []  # List to hold chunks of dataframes\n\n# Read the file in chunks\nfor chunk in pd.read_csv('/mnt/data/District_Platoon_Staffing_Exceptions.csv', chunksize=chunk_size):\n    # Process each chunk to extract necessary information\n    chunk['Exception Date'] = pd.to_datetime(chunk['Exception Date'])\n    chunk = chunk.groupby(chunk['Exception Date'].dt.date).agg({'# of Hours': 'sum'}).reset_index()\n    chunks.append(chunk)\n\n# Concatenate chunks into a single dataframe\nstaffing_exceptions_aggregated = pd.concat(chunks, ignore_index=True)\n\n# Display the first few rows of the aggregated exceptions data\ndisplay(staffing_exceptions_aggregated.head())\n\n# Now we will merge this with the calls data and perform the correlation analysis\n# This will be done in the next cell to keep the operations modular and manageable.",
      "outputs": []
    },
    {
      "id": "3c4333d0-f143-4a65-93a9-f2a74634612b",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "9cffdc44-cc4c-43e1-ba73-53d71ffdfc5e"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:11:41.536167+00:00",
          "start_time": "2023-11-09T17:11:41.287563+00:00"
        }
      },
      "execution_count": null,
      "source": "# Load the Staffing Exceptions data in chunks due to its large size\nchunk_size = 50000  # Define the chunk size\nchunks = []  # List to hold chunks of dataframes\n\n# Read the file in chunks\nfor chunk in pd.read_csv('District_Platoon_Staffing_Exceptions.csv', chunksize=chunk_size):\n    # Process each chunk to extract necessary information\n    chunk['Exception Date'] = pd.to_datetime(chunk['Exception Date'])\n    chunk = chunk.groupby(chunk['Exception Date'].dt.date).agg({'# of Hours': 'sum'}).reset_index()\n    chunks.append(chunk)\n\n# Concatenate chunks into a single dataframe\nstaffing_exceptions_aggregated = pd.concat(chunks, ignore_index=True)\n\n# Display the first few rows of the aggregated exceptions data\nstaffing_exceptions_aggregated.head()",
      "outputs": []
    },
    {
      "id": "4f878219-3323-4b44-9433-52675badd991",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "841942d7-cb23-4acb-ad1e-0d1eea1f3179"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:11:58.493865+00:00",
          "start_time": "2023-11-09T17:11:57.816919+00:00"
        },
        "datalink": {
          "61b32ffc-e8ad-4eeb-8d14-deb550456c43": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 10,
              "orig_num_rows": 5,
              "orig_size_bytes": 440,
              "truncated_num_cols": 10,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 440,
              "truncated_string_columns": []
            },
            "display_id": "61b32ffc-e8ad-4eeb-8d14-deb550456c43",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-11-09T17:11:58.326430",
            "user_variable_name": "df_sample",
            "variable_name": "df_sample"
          }
        }
      },
      "execution_count": null,
      "source": "# Inspect the first few rows of the CSV file to understand its structure\nimport pandas as pd\n\n# Load the first few rows of the data\nstaffing_exceptions_sample = pd.read_csv('District_Platoon_Staffing_Exceptions.csv', nrows=5)\n\n# Display the first few rows to inspect the column names\nstaffing_exceptions_sample",
      "outputs": []
    },
    {
      "id": "d085ecd4-e6de-4cad-bc7d-03786d712420",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "66dd15fc-789a-491f-ad48-a5c32d8387d8"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:12:18.001368+00:00",
          "start_time": "2023-11-09T17:12:17.392127+00:00"
        },
        "datalink": {
          "a8a13e57-4c33-4d7d-afa1-f40aa55bc8aa": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 2,
              "orig_num_rows": 5,
              "orig_size_bytes": 120,
              "truncated_num_cols": 2,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 120,
              "truncated_string_columns": []
            },
            "display_id": "a8a13e57-4c33-4d7d-afa1-f40aa55bc8aa",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-11-09T17:12:17.831923",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_b50b96bfd92f4390be1ef40dcc0f132d"
          }
        }
      },
      "execution_count": null,
      "source": "# Load the Staffing Exceptions data in chunks due to its large size\nchunk_size = 50000  # Define the chunk size\nchunks = []  # List to hold chunks of dataframes\n\n# Read the file in chunks\nfor chunk in pd.read_csv('District_Platoon_Staffing_Exceptions.csv', chunksize=chunk_size):\n    # Process each chunk to extract necessary information\n    # Combine year, month, and day columns to create a 'Date' column\n    chunk['Date'] = pd.to_datetime(chunk[['year', 'month', 'day']])\n    # Group by the new 'Date' column and sum the '# of Hours'\n    chunk = chunk.groupby('Date').agg({'# of Hours': 'sum'}).reset_index()\n    chunks.append(chunk)\n\n# Concatenate chunks into a single dataframe\nstaffing_exceptions_aggregated = pd.concat(chunks, ignore_index=True)\n\n# Display the first few rows of the aggregated exceptions data\nstaffing_exceptions_aggregated.head()",
      "outputs": []
    },
    {
      "id": "7f158dc4-3e63-4682-bc3c-7886e1e01c2b",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "ed94f506-4d95-4970-a25d-915db5de1c5b"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:12:37.312415+00:00",
          "start_time": "2023-11-09T17:12:37.061470+00:00"
        }
      },
      "execution_count": null,
      "source": "# Merge the aggregated staffing exceptions data with the calls data\n# First, load the calls data\n\n# Define a function to parse dates\nparse_dates = lambda x: pd.datetime.strptime(x, '%Y-%m-%d')\n\ncalls_data = pd.read_csv('Calls_for_Service.csv', parse_dates=['Date'], date_parser=parse_dates)\n\n# Group the calls data by date and count the number of calls\ncalls_data_grouped = calls_data.groupby('Date').size().reset_index(name='Number of Calls')\n\n# Merge the two datasets on the 'Date' column\nmerged_data = pd.merge(staffing_exceptions_aggregated, calls_data_grouped, on='Date', how='outer')\n\n# Fill NaN values with 0 for days with no staffing exceptions\nmerged_data.fillna({'# of Hours': 0, 'Number of Calls': 0}, inplace=True)\n\n# Calculate the correlation between the number of hours of staffing exceptions and the number of calls\ncorrelation = merged_data[['# of Hours', 'Number of Calls']].corr()\n\n# Display the correlation matrix\nmerged_data.head(), correlation",
      "outputs": []
    },
    {
      "id": "7950c56a-ff50-4780-a51d-0252105bfc7c",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "657c4c51-92c5-4aba-b51a-25ade66ff18d"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:12:59.990065+00:00",
          "start_time": "2023-11-09T17:12:59.336430+00:00"
        },
        "datalink": {
          "41e6869a-65e8-4ac3-b96c-53178b222e7f": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 9,
              "orig_num_rows": 5,
              "orig_size_bytes": 400,
              "truncated_num_cols": 9,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 400,
              "truncated_string_columns": []
            },
            "display_id": "41e6869a-65e8-4ac3-b96c-53178b222e7f",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-11-09T17:12:59.826502",
            "user_variable_name": "df_sample_calls",
            "variable_name": "df_sample_calls"
          }
        }
      },
      "execution_count": null,
      "source": "# Inspect the first few rows of the 'Calls_for_Service.csv' to understand its structure\nimport pandas as pd\n\n# Load the first few rows of the data\ncalls_sample = pd.read_csv('Calls_for_Service.csv', nrows=5)\n\n# Display the first few rows to inspect the column names\ncalls_sample",
      "outputs": []
    },
    {
      "id": "72126338-f31e-46ee-8922-d1fa9ac20559",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "237652e9-d3c7-4377-baad-df8fd6532164"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:13:23.737781+00:00",
          "start_time": "2023-11-09T17:13:22.750172+00:00"
        },
        "datalink": {
          "a609ab15-78c7-475f-8015-f170146e3f9d": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 2,
              "orig_num_rows": 5,
              "orig_size_bytes": 120,
              "truncated_num_cols": 2,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 120,
              "truncated_string_columns": []
            },
            "display_id": "a609ab15-78c7-475f-8015-f170146e3f9d",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-11-09T17:13:23.573452",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_dc109be98c19401dabaaef2132e5b203"
          }
        }
      },
      "execution_count": null,
      "source": "# Load the Calls for Service data in chunks due to its large size\nchunk_size = 50000  # Define the chunk size\nchunks = []  # List to hold chunks of dataframes\n\n# Read the file in chunks\nfor chunk in pd.read_csv('Calls_for_Service.csv', chunksize=chunk_size):\n    # Process each chunk to extract necessary information\n    # Combine year, month, and day columns to create a 'Date' column\n    chunk['Date'] = pd.to_datetime(chunk[['year', 'month', 'day']])\n    # Group by the new 'Date' column and count the number of calls\n    chunk = chunk.groupby('Date').size().reset_index(name='Number of Calls')\n    chunks.append(chunk)\n\n# Concatenate chunks into a single dataframe\ncalls_data_aggregated = pd.concat(chunks, ignore_index=True)\n\n# Display the first few rows of the aggregated calls data\ncalls_data_aggregated.head()",
      "outputs": []
    },
    {
      "id": "c8e62379-fc05-481d-b7fd-cd3b2cf68f57",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "64f8eb1b-5cae-48d9-a82b-beaca7d92fc4"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:13:42.076567+00:00",
          "start_time": "2023-11-09T17:13:41.907494+00:00"
        }
      },
      "execution_count": null,
      "source": "# Merge the aggregated calls data with the staffing exceptions data\n\n# Merge the two datasets on the 'Date' column\nmerged_data = pd.merge(calls_data_aggregated, staffing_exceptions_aggregated, on='Date', how='outer')\n\n# Fill NaN values with 0 for days with no staffing exceptions or calls\nmerged_data.fillna({'# of Hours': 0, 'Number of Calls': 0}, inplace=True)\n\n# Calculate the correlation between the number of hours of staffing exceptions and the number of calls\ncorrelation = merged_data[['# of Hours', 'Number of Calls']].corr()\n\n# Display the correlation matrix\nmerged_data.head(), correlation",
      "outputs": []
    },
    {
      "id": "2e41ceee-2408-433e-839d-bd57e0de4ffe",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "d81894c5-7ab5-4d42-ae31-e7d567dd2a2b"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:17:43.074486+00:00",
          "start_time": "2023-11-09T17:17:42.834543+00:00"
        }
      },
      "execution_count": null,
      "source": "# Load the Staffing Exceptions data in chunks due to its large size\nchunk_size = 50000  # Define the chunk size\nchunks = []  # List to hold chunks of dataframes\n\n# Read the file in chunks\nfor chunk in pd.read_csv('/mnt/data/District_Platoon_Staffing_Exceptions.csv', chunksize=chunk_size):\n    # Convert 'Date' to datetime and extract the date part\n    chunk['Date'] = pd.to_datetime(chunk['Date']).dt.date\n    # Aggregate the data by 'Date' to get the total number of hours of exceptions for each day\n    daily_exceptions = chunk.groupby('Date')['# of Hours'].sum().reset_index()\n    chunks.append(daily_exceptions)\n\n# Concatenate chunks into a single dataframe\nstaffing_exceptions_aggregated = pd.concat(chunks, ignore_index=True)\n# Aggregate again in case the same date appears in different chunks\nstaffing_exceptions_aggregated = staffing_exceptions_aggregated.groupby('Date')['# of Hours'].sum().reset_index()\n\n# Display the first few rows of the aggregated exceptions dataframe\ndisplay(staffing_exceptions_aggregated.head())",
      "outputs": []
    },
    {
      "id": "5f664561-c640-4273-bfee-0b8b3b0ba877",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "1636532c-acf5-40f0-97f6-45ccb908dc9a"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:18:22.342995+00:00",
          "start_time": "2023-11-09T17:18:22.100909+00:00"
        }
      },
      "execution_count": null,
      "source": "# Load the Staffing Exceptions data in chunks due to its large size\nchunk_size = 50000  # Define the chunk size\nchunks = []  # List to hold chunks of dataframes\n\n# Read the file in chunks\nfor chunk in pd.read_csv('District_Platoon_Staffing_Exceptions.csv', chunksize=chunk_size):\n    # Convert 'Date' to datetime and extract the date part\n    chunk['Date'] = pd.to_datetime(chunk['Date']).dt.date\n    # Aggregate the data by 'Date' to get the total number of hours of exceptions for each day\n    daily_exceptions = chunk.groupby('Date')['# of Hours'].sum().reset_index()\n    chunks.append(daily_exceptions)\n\n# Concatenate chunks into a single dataframe\nstaffing_exceptions_aggregated = pd.concat(chunks, ignore_index=True)\n# Aggregate again in case the same date appears in different chunks\nstaffing_exceptions_aggregated = staffing_exceptions_aggregated.groupby('Date')['# of Hours'].sum().reset_index()\n\n# Display the first few rows of the aggregated exceptions dataframe\ndisplay(staffing_exceptions_aggregated.head())",
      "outputs": []
    },
    {
      "id": "fb55781e-50cc-466a-9746-5bb507b70d31",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "8e2f771b-a2cc-4b30-9b2f-aab2760889a0"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:19:02.930476+00:00",
          "start_time": "2023-11-09T17:19:02.736525+00:00"
        }
      },
      "execution_count": null,
      "source": "# Load the first chunk of the Staffing Exceptions data to inspect the column names\nchunk_size = 50000  # Define the chunk size\nfirst_chunk = pd.read_csv('District_Platoon_Staffing_Exceptions.csv', chunksize=chunk_size).__next__()\n\n# Display the column names\nfirst_chunk.columns.tolist()",
      "outputs": []
    },
    {
      "id": "8c1b3f0b-0a64-40c5-a1a0-322b2f281e16",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "d3d4ebfc-a126-464d-bdd0-4ec68db92f8d"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:19:49.476123+00:00",
          "start_time": "2023-11-09T17:19:49.232963+00:00"
        }
      },
      "execution_count": null,
      "source": "# Load the Staffing Exceptions data in chunks due to its large size\nchunk_size = 50000  # Define the chunk size\nchunks = []  # List to hold chunks of dataframes\n\n# Read the file in chunks\nfor chunk in pd.read_csv('/mnt/data/District_Platoon_Staffing_Exceptions.csv', chunksize=chunk_size):\n    # Process each chunk to extract the necessary information\n    chunk['Exception Date'] = pd.to_datetime(chunk['Exception Date'])\n    chunk = chunk.groupby(chunk['Exception Date'].dt.date).size().reset_index(name='exceptions_count')\n    chunks.append(chunk)\n\n# Concatenate chunks into a single dataframe\nstaffing_exceptions_aggregated = pd.concat(chunks, ignore_index=True)\n\n# Display the first few rows of the aggregated exceptions data\ndisplay(staffing_exceptions_aggregated.head())",
      "outputs": []
    },
    {
      "id": "76b0a37a-bb7b-440a-90b1-e6cf56ecc86c",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "fa1d2fd6-8b84-4eed-8a76-ff15afed834f"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:20:17.366136+00:00",
          "start_time": "2023-11-09T17:20:17.125038+00:00"
        }
      },
      "execution_count": null,
      "source": "# Load the Staffing Exceptions data in chunks due to its large size\nchunk_size = 50000  # Define the chunk size\nchunks = []  # List to hold chunks of dataframes\n\n# Read the file in chunks\nfor chunk in pd.read_csv('District_Platoon_Staffing_Exceptions.csv', chunksize=chunk_size):\n    # Process each chunk to extract the necessary information\n    chunk['Exception Date'] = pd.to_datetime(chunk['Exception Date'])\n    chunk = chunk.groupby(chunk['Exception Date'].dt.date).size().reset_index(name='exceptions_count')\n    chunks.append(chunk)\n\n# Concatenate chunks into a single dataframe\nstaffing_exceptions_aggregated = pd.concat(chunks, ignore_index=True)\n\n# Display the first few rows of the aggregated exceptions data\nstaffing_exceptions_aggregated.head()",
      "outputs": []
    },
    {
      "id": "02ab9d58-45f0-401c-8205-4de366c04f71",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "556cb358-6bce-42f7-b1a5-3b7fce4b8540"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:20:39.573420+00:00",
          "start_time": "2023-11-09T17:20:38.873867+00:00"
        },
        "datalink": {
          "cdcc6b3f-1a9c-4044-9a39-3fdd9abb2dc2": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 10,
              "orig_num_rows": 5,
              "orig_size_bytes": 440,
              "truncated_num_cols": 10,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 440,
              "truncated_string_columns": []
            },
            "display_id": "cdcc6b3f-1a9c-4044-9a39-3fdd9abb2dc2",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-11-09T17:20:39.412092",
            "user_variable_name": "df_sample",
            "variable_name": "df_sample"
          }
        }
      },
      "execution_count": null,
      "source": "# Let's read the first few lines of the 'District_Platoon_Staffing_Exceptions.csv' to understand its structure\nimport pandas as pd\n\n# Read in the first few rows of the dataframe\ndf_head = pd.read_csv('District_Platoon_Staffing_Exceptions.csv', nrows=5)\n\n# Display the first few rows\ndf_head",
      "outputs": []
    },
    {
      "id": "1af0bbb4-511f-45da-a755-01bc5df46bd3",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "bbb3674a-1bcc-43e5-a865-baac71d794e1"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:20:56.615704+00:00",
          "start_time": "2023-11-09T17:20:55.977362+00:00"
        },
        "datalink": {
          "9a277a88-85ad-40fe-bcf7-275fd998847a": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 2,
              "orig_num_rows": 5,
              "orig_size_bytes": 120,
              "truncated_num_cols": 2,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 120,
              "truncated_string_columns": []
            },
            "display_id": "9a277a88-85ad-40fe-bcf7-275fd998847a",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-11-09T17:20:56.452240",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_40cc421a269f4daab0d4731a58eebe3b"
          }
        }
      },
      "execution_count": null,
      "source": "# Load the Staffing Exceptions data in chunks due to its large size\nchunk_size = 50000  # Define the chunk size\nchunks = []  # List to hold chunks of dataframes\n\n# Read the file in chunks\nfor chunk in pd.read_csv('District_Platoon_Staffing_Exceptions.csv', chunksize=chunk_size):\n    # Process each chunk to extract the necessary information\n    # Combine year, month, and day columns to create a 'Date' column\n    chunk['Date'] = pd.to_datetime(chunk[['year', 'month', 'day']])\n    # Group by the new 'Date' column and sum the '# of Hours'\n    chunk = chunk.groupby('Date')['# of Hours'].sum().reset_index()\n    chunks.append(chunk)\n\n# Concatenate chunks into a single dataframe\nstaffing_exceptions_aggregated = pd.concat(chunks, ignore_index=True)\n\n# Display the first few rows of the aggregated exceptions data\nstaffing_exceptions_aggregated.head()",
      "outputs": []
    },
    {
      "id": "62f928c8-4dd2-4a69-a052-7c8c7c8624bc",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "9c179ab3-557a-4921-8963-d838392afcfc"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:21:10.945124+00:00",
          "start_time": "2023-11-09T17:21:10.754978+00:00"
        }
      },
      "execution_count": null,
      "source": "# Merge the calls data with the staffing exceptions data on the date\ncalls_and_staffing = pd.merge(calls_aggregated, staffing_exceptions_aggregated, left_on='Date', right_on='Date', how='left')\n\n# Fill NaN values with 0 for days where there are no staffing exceptions\n# This assumes that a NaN value means no exceptions were recorded for that day\ncalls_and_staffing['# of Hours'].fillna(0, inplace=True)\n\n# Display the first few rows of the merged dataframe\ncalls_and_staffing.head()",
      "outputs": []
    },
    {
      "id": "89ae0ea2-d64a-4a02-a020-0ce5fa0f648c",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "926c92bb-025f-48bc-b45e-dabd316f99e7"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:21:30.654045+00:00",
          "start_time": "2023-11-09T17:21:30.435295+00:00"
        }
      },
      "execution_count": null,
      "source": "# Ensure the 'Date' column in calls_aggregated is of datetime type for merging\ncalls_aggregated['Date'] = pd.to_datetime(calls_aggregated['Date'])\n\n# Merge the calls data with the staffing exceptions data on the date\ncalls_and_staffing = pd.merge(calls_aggregated, staffing_exceptions_aggregated, left_on='Date', right_on='Date', how='left')\n\n# Fill NaN values with 0 for days where there are no staffing exceptions\n# This assumes that a NaN value means no exceptions were recorded for that day\ncalls_and_staffing['# of Hours'].fillna(0, inplace=True)\n\n# Display the first few rows of the merged dataframe\ncalls_and_staffing.head()",
      "outputs": []
    },
    {
      "id": "fb2b286d-4df1-4757-991a-cd26c27c4b47",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "a793cf69-4150-41c8-bea0-ea8f5bd18143"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:22:26.667242+00:00",
          "start_time": "2023-11-09T17:22:26.335220+00:00"
        }
      },
      "execution_count": null,
      "source": "# Check if 'Date' column exists in calls_aggregated and staffing_exceptions_aggregated\ntry:\n    calls_aggregated['Date']\nexcept KeyError:\n    # If 'Date' column doesn't exist, create it from 'year', 'month', and 'day'\n    calls_aggregated['Date'] = pd.to_datetime(calls_aggregated[['year', 'month', 'day']])\n\ntry:\n    staffing_exceptions_aggregated['Date']\nexcept KeyError:\n    # If 'Date' column doesn't exist, create it from 'year', 'month', and 'day'\n    staffing_exceptions_aggregated['Date'] = pd.to_datetime(staffing_exceptions_aggregated[['year', 'month', 'day']])\n\n# Now that we have ensured both dataframes have a 'Date' column, attempt the merge again\ncalls_and_staffing = pd.merge(calls_aggregated, staffing_exceptions_aggregated, left_on='Date', right_on='Date', how='left')\n\n# Fill NaN values with 0 for days where there are no staffing exceptions\n# This assumes that a NaN value means no exceptions were recorded for that day\ncalls_and_staffing['exceptions_count'].fillna(0, inplace=True)\n\n# Display the first few rows of the merged dataframe\ncalls_and_staffing.head()",
      "outputs": []
    },
    {
      "id": "88760ab2-b32a-492e-a7ce-b7dc1d090f4e",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "3da38cfd-40d9-441a-b129-3a625e429c25"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:22:48.294813+00:00",
          "start_time": "2023-11-09T17:22:48.137352+00:00"
        }
      },
      "execution_count": null,
      "source": "# Display column names for both dataframes to understand their structure\nprint('Calls Aggregated Columns:\\n', calls_aggregated.columns)\nprint('\\nStaffing Exceptions Aggregated Columns:\\n', staffing_exceptions_aggregated.columns)",
      "outputs": []
    },
    {
      "id": "569ab1b8-82b9-4e02-a71d-7b71b04d8aea",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "55a3f8d5-52cc-48fc-b9cf-a5cd8928165d"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:23:12.285501+00:00",
          "start_time": "2023-11-09T17:23:11.636505+00:00"
        },
        "datalink": {
          "74fc150d-0771-4a26-9d73-272627f513e0": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 4,
              "orig_num_rows": 5,
              "orig_size_bytes": 200,
              "truncated_num_cols": 4,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 200,
              "truncated_string_columns": []
            },
            "display_id": "74fc150d-0771-4a26-9d73-272627f513e0",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-11-09T17:23:12.120986",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_21d7d49240624bc2b51d911ddd51cfad"
          }
        }
      },
      "execution_count": null,
      "source": "# Merge the calls data with the staffing exceptions data on the correct date columns\ncalls_and_staffing = pd.merge(calls_aggregated, staffing_exceptions_aggregated, left_on='Call Date', right_on='Date', how='left')\n\n# Fill NaN values with 0 for days where there are no staffing exceptions\n# This assumes that a NaN value means no exceptions were recorded for that day\ncalls_and_staffing['# of Hours'].fillna(0, inplace=True)\n\n# Display the first few rows of the merged dataframe\ncalls_and_staffing.head()",
      "outputs": []
    },
    {
      "id": "09eedf10-5217-49b0-9d5e-c675d2644821",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "20125af4-59fd-45e1-bff5-248cb622929a"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:23:32.876140+00:00",
          "start_time": "2023-11-09T17:23:32.718849+00:00"
        }
      },
      "execution_count": null,
      "source": "# Calculate the correlation coefficient between 'Total Calls' and '# of Hours' to understand the relationship\ncorrelation = calls_and_staffing['Total Calls'].corr(calls_and_staffing['# of Hours'])\nprint('Correlation coefficient:', correlation)",
      "outputs": []
    },
    {
      "id": "b0703c82-3c4c-4444-bcc7-db73145f258b",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "974210e1-07de-4f61-88fe-45047998f987"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:23:41.347623+00:00",
          "start_time": "2023-11-09T17:23:41.110950+00:00"
        }
      },
      "execution_count": null,
      "source": "# Define a function to process chunks of the staffing exceptions data\ndef process_staffing_exceptions_chunk(chunk):\n    # Convert 'Date' to datetime and extract the date part\n    chunk['Date'] = pd.to_datetime(chunk['Date']).dt.date\n    # Group by 'Date' and sum the '# of Hours'\n    return chunk.groupby('Date')['# of Hours'].sum().reset_index()\n\n# Initialize an empty dataframe to store aggregated results\nstaffing_exceptions_aggregated = pd.DataFrame()\n\n# Define the chunk size\nchunk_size = 50000\n\n# Read the staffing exceptions file in chunks and process each chunk\nfor chunk in pd.read_csv('/mnt/data/District_Platoon_Staffing_Exceptions.csv', chunksize=chunk_size):\n    staffing_exceptions_aggregated = staffing_exceptions_aggregated.append(process_staffing_exceptions_chunk(chunk))\n\n# Group by 'Date' again in case there are overlapping dates between chunks\nstaffing_exceptions_aggregated = staffing_exceptions_aggregated.groupby('Date').sum().reset_index()\n\n# Display the first few rows of the aggregated staffing exceptions data\ndisplay(staffing_exceptions_aggregated.head())",
      "outputs": []
    },
    {
      "id": "424f6cbe-55a5-438e-accf-94223eff0f99",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "b0fe8999-cce8-4607-afc8-331b62f8f330"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:24:14.096978+00:00",
          "start_time": "2023-11-09T17:24:13.834990+00:00"
        }
      },
      "execution_count": null,
      "source": "# Define a function to process chunks of the staffing exceptions data\ndef process_staffing_exceptions_chunk(chunk):\n    # Convert 'Date' to datetime and extract the date part\n    chunk['Date'] = pd.to_datetime(chunk['Date']).dt.date\n    # Group by 'Date' and sum the '# of Hours'\n    return chunk.groupby('Date')['# of Hours'].sum().reset_index()\n\n# Initialize an empty dataframe to store aggregated results\nstaffing_exceptions_aggregated = pd.DataFrame()\n\n# Define the chunk size\nchunk_size = 50000\n\n# Read the staffing exceptions file in chunks and process each chunk\nfor chunk in pd.read_csv('District_Platoon_Staffing_Exceptions.csv', chunksize=chunk_size):\n    staffing_exceptions_aggregated = staffing_exceptions_aggregated.append(process_staffing_exceptions_chunk(chunk))\n\n# Group by 'Date' again in case there are overlapping dates between chunks\nstaffing_exceptions_aggregated = staffing_exceptions_aggregated.groupby('Date').sum().reset_index()\n\n# Display the first few rows of the aggregated staffing exceptions data\ndisplay(staffing_exceptions_aggregated.head())",
      "outputs": []
    },
    {
      "id": "5390fd65-99a1-4b04-bdbe-2329130863d5",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "fbe3f913-2015-4502-b1a4-db23d724a740"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:24:31.242225+00:00",
          "start_time": "2023-11-09T17:24:30.528962+00:00"
        },
        "datalink": {
          "7550721f-b514-4068-aafc-7a5b8fd7cc9e": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 10,
              "orig_num_rows": 5,
              "orig_size_bytes": 440,
              "truncated_num_cols": 10,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 440,
              "truncated_string_columns": []
            },
            "display_id": "7550721f-b514-4068-aafc-7a5b8fd7cc9e",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-11-09T17:24:31.077340",
            "user_variable_name": "df_sample",
            "variable_name": "df_sample"
          }
        }
      },
      "execution_count": null,
      "source": "# Load a small sample of the staffing exceptions data to check the structure and column names\nstaffing_exceptions_sample = pd.read_csv('District_Platoon_Staffing_Exceptions.csv', nrows=5)\nstaffing_exceptions_sample.head()",
      "outputs": []
    },
    {
      "id": "8d12605c-5648-4bfd-b9c3-819fbcf318a3",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "e13f9cc8-5aef-4c9d-b3f7-e2e8f7f6cfeb"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:24:59.366345+00:00",
          "start_time": "2023-11-09T17:24:58.661901+00:00"
        },
        "datalink": {
          "ecbb2bab-5325-452b-96d8-8073dbd2ff02": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 2,
              "orig_num_rows": 5,
              "orig_size_bytes": 120,
              "truncated_num_cols": 2,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 120,
              "truncated_string_columns": []
            },
            "display_id": "ecbb2bab-5325-452b-96d8-8073dbd2ff02",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-11-09T17:24:59.202816",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_6d139da1e7a648e788965a0c193e28dc"
          }
        }
      },
      "execution_count": null,
      "source": "# Define a function to process chunks of the staffing exceptions data\ndef process_staffing_exceptions_chunk(chunk):\n    # Combine year, month, and day columns to create a 'Date' column\n    chunk['Date'] = pd.to_datetime(chunk[['year', 'month', 'day']])\n    # Group by 'Date' and sum the '# of Hours'\n    return chunk.groupby('Date')['# of Hours'].sum().reset_index()\n\n# Initialize an empty dataframe to store aggregated results\nstaffing_exceptions_aggregated = pd.DataFrame()\n\n# Define the chunk size\nchunk_size = 50000\n\n# Read the staffing exceptions file in chunks and process each chunk\nfor chunk in pd.read_csv('District_Platoon_Staffing_Exceptions.csv', chunksize=chunk_size):\n    staffing_exceptions_aggregated = staffing_exceptions_aggregated.append(process_staffing_exceptions_chunk(chunk))\n\n# Group by 'Date' again in case there are overlapping dates between chunks\nstaffing_exceptions_aggregated = staffing_exceptions_aggregated.groupby('Date').sum().reset_index()\n\n# Display the first few rows of the aggregated staffing exceptions data\ndisplay(staffing_exceptions_aggregated.head())",
      "outputs": []
    },
    {
      "id": "58ff61c4-6a2b-43b1-ae25-c5eef4f73605",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "b4ee4c50-45aa-4501-91f4-c6920daf3e87"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:25:11.347975+00:00",
          "start_time": "2023-11-09T17:25:11.177213+00:00"
        }
      },
      "execution_count": null,
      "source": "# Merge the calls for service data with the staffing exceptions data on the 'Date' column\ncalls_staffing_merged = calls_for_service_aggregated.merge(staffing_exceptions_aggregated, on='Date', how='inner')\n\n# Calculate the correlation between 'Total Calls' and '# of Hours'\ncorrelation = calls_staffing_merged[['Total Calls', '# of Hours']].corr()\n\n# Display the correlation matrix\ncorrelation",
      "outputs": []
    },
    {
      "id": "c7275957-970b-4151-be4e-3f53ffc5c79a",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "73a4765a-171e-4f96-9673-591c845ec399"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:25:33.375707+00:00",
          "start_time": "2023-11-09T17:25:32.605514+00:00"
        },
        "datalink": {
          "a03c32c8-f436-4be2-87c5-12b1c7b387c0": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 9,
              "orig_num_rows": 5,
              "orig_size_bytes": 400,
              "truncated_num_cols": 9,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 400,
              "truncated_string_columns": []
            },
            "display_id": "a03c32c8-f436-4be2-87c5-12b1c7b387c0",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-11-09T17:25:33.210749",
            "user_variable_name": "df_sample_calls",
            "variable_name": "df_sample_calls"
          }
        }
      },
      "execution_count": null,
      "source": "# Load a small sample of the calls for service data to check the structure and column names\ncalls_for_service_sample = pd.read_csv('Calls_for_Service.csv', nrows=5)\ncalls_for_service_sample.head()",
      "outputs": []
    },
    {
      "id": "95c29f40-6707-468d-9670-fca811fb542e",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "d887976e-4c0d-48ad-9240-1771894c2488"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:25:58.950130+00:00",
          "start_time": "2023-11-09T17:25:57.908502+00:00"
        },
        "datalink": {
          "89df9371-f912-4bc5-997e-a14775393dde": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 2,
              "orig_num_rows": 5,
              "orig_size_bytes": 120,
              "truncated_num_cols": 2,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 120,
              "truncated_string_columns": []
            },
            "display_id": "89df9371-f912-4bc5-997e-a14775393dde",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-11-09T17:25:58.785461",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_5db679c2b95940f294440e9fc3d6da82"
          }
        }
      },
      "execution_count": null,
      "source": "# Define a function to process chunks of the calls for service data\ndef process_calls_for_service_chunk(chunk):\n    # Combine year, month, and day columns to create a 'Date' column\n    chunk['Date'] = pd.to_datetime(chunk[['year', 'month', 'day']])\n    # Count the number of calls for each date\n    return chunk.groupby('Date').size().reset_index(name='Total Calls')\n\n# Initialize an empty dataframe to store aggregated results\ncalls_for_service_aggregated = pd.DataFrame()\n\n# Define the chunk size\nchunk_size = 50000\n\n# Read the calls for service file in chunks and process each chunk\nfor chunk in pd.read_csv('Calls_for_Service.csv', chunksize=chunk_size):\n    calls_for_service_aggregated = calls_for_service_aggregated.append(process_calls_for_service_chunk(chunk))\n\n# Group by 'Date' again in case there are overlapping dates between chunks\ncalls_for_service_aggregated = calls_for_service_aggregated.groupby('Date').sum().reset_index()\n\n# Display the first few rows of the aggregated calls for service data\ndisplay(calls_for_service_aggregated.head())",
      "outputs": []
    },
    {
      "id": "57d0de8a-f387-4bd8-891f-d53770e5a101",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "58964bcb-e3b9-49b9-8f22-3d0f44bd558b"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:26:14.933281+00:00",
          "start_time": "2023-11-09T17:26:14.256903+00:00"
        },
        "datalink": {
          "3c5193d6-9ff4-4525-b8d8-795c31a55c6c": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": false,
              "orig_num_cols": 2,
              "orig_num_rows": 2,
              "orig_size_bytes": 48,
              "truncated_num_cols": 2,
              "truncated_num_rows": 2,
              "truncated_size_bytes": 48,
              "truncated_string_columns": []
            },
            "display_id": "3c5193d6-9ff4-4525-b8d8-795c31a55c6c",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-11-09T17:26:14.773138",
            "user_variable_name": "correlation",
            "variable_name": "correlation"
          }
        }
      },
      "execution_count": null,
      "source": "# Merge the calls for service data with the staffing exceptions data on the 'Date' column\ncalls_staffing_merged = calls_for_service_aggregated.merge(staffing_exceptions_aggregated, on='Date', how='inner')\n\n# Calculate the correlation between 'Total Calls' and '# of Hours'\ncorrelation = calls_staffing_merged[['Total Calls', '# of Hours']].corr()\n\n# Display the correlation matrix\ncorrelation",
      "outputs": []
    },
    {
      "id": "767515f0-db77-4c01-acd5-f7b4f476e0bc",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "ac729693-8f69-4c60-8535-4913e33a65b7"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:31:11.671480+00:00",
          "start_time": "2023-11-09T17:31:10.896913+00:00"
        },
        "datalink": {
          "43028dc1-7aea-4fe9-84ba-588faa96ddf3": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 2,
              "orig_num_rows": 1,
              "orig_size_bytes": 24,
              "truncated_num_cols": 2,
              "truncated_num_rows": 1,
              "truncated_size_bytes": 24,
              "truncated_string_columns": []
            },
            "display_id": "43028dc1-7aea-4fe9-84ba-588faa96ddf3",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-11-09T17:31:11.511399",
            "user_variable_name": "chunk",
            "variable_name": "chunk"
          }
        }
      },
      "execution_count": null,
      "source": "# Load the District_Platoon_Staffing_Exceptions data in chunks due to its large size\nchunk_size = 50000  # Define the chunk size\nchunks = []  # List to hold chunks of dataframes\n\n# Read the file in chunks\nfor chunk in pd.read_csv('District_Platoon_Staffing_Exceptions.csv', chunksize=chunk_size):\n    # Process each chunk to extract necessary information\n    chunk['Exception Start Time'] = pd.to_datetime(chunk['Exception Start Time'])\n    chunk = chunk.groupby(chunk['Exception Start Time'].dt.date).agg({'# of Hours': 'sum'}).reset_index()\n    chunks.append(chunk)\n\n# Concatenate chunks into a single dataframe\nstaffing_exceptions_aggregated = pd.concat(chunks, ignore_index=True)\n\n# Display the first few rows of the aggregated exceptions data\ndisplay(staffing_exceptions_aggregated.head())",
      "outputs": []
    },
    {
      "id": "42f25b60-45c8-43d7-9af4-e8add1222254",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "0d1400a0-e960-4a02-a265-971b2fd8819f"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:30:32.381096+00:00",
          "start_time": "2023-11-09T17:30:32.205388+00:00"
        }
      },
      "execution_count": null,
      "source": "# Load the first chunk of the District_Platoon_Staffing_Exceptions data to inspect the column names\nchunk_size = 50000  # Define the chunk size\n\n# Read the first chunk of the file\nfirst_chunk = pd.read_csv('District_Platoon_Staffing_Exceptions.csv', chunksize=chunk_size).__next__()\n\n# Display the column names of the first chunk\ncolumn_names = first_chunk.columns.tolist()\ndisplay(column_names)",
      "outputs": []
    },
    {
      "id": "bd344182-43e0-45fd-8913-beb5278de58f",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "d43cbeed-a7c7-4ab5-a904-7871f77ff54d"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:31:46.793842+00:00",
          "start_time": "2023-11-09T17:31:46.614499+00:00"
        }
      },
      "execution_count": null,
      "source": "# Merge the calls for service data with the staffing exceptions data on the date\ncalls_for_service_df['Call Date'] = pd.to_datetime(calls_for_service_df['Call Date'])\nmerged_data = pd.merge(calls_for_service_df, staffing_exceptions_aggregated, left_on='Call Date', right_on='Exception Start Time', how='inner')\n\n# Calculate the total number of calls per day\ncalls_per_day = merged_data.groupby('Call Date').size().reset_index(name='calls_count')\n\n# Merge the calls per day with the exceptions hours per day\ndemand_staffing_correlation = pd.merge(calls_per_day, staffing_exceptions_aggregated, left_on='Call Date', right_on='Exception Start Time', how='inner')\n\n# Calculate the correlation\ncorrelation = demand_staffing_correlation[['calls_count', '# of Hours']].corr()\n\n# Display the correlation matrix\ndisplay(correlation)\n\n# Visualize the correlation with a scatter plot\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=demand_staffing_correlation, x='calls_count', y='# of Hours')\nplt.title('Correlation between Call Demand and Staffing Exceptions Hours')\nplt.xlabel('Total Calls per Day')\nplt.ylabel('Total Staffing Exceptions Hours per Day')\nplt.show()",
      "outputs": []
    },
    {
      "id": "cd8a3bb2-6cb7-4f40-a369-fb5d00cb3176",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "548a1bf2-e864-4830-92eb-d39abedfa14a"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:32:17.013110+00:00",
          "start_time": "2023-11-09T17:32:16.102469+00:00"
        },
        "datalink": {
          "1be33372-8869-4760-b05b-cb5d678b1a4c": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 3,
              "orig_num_rows": 5,
              "orig_size_bytes": 160,
              "truncated_num_cols": 3,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 160,
              "truncated_string_columns": []
            },
            "display_id": "1be33372-8869-4760-b05b-cb5d678b1a4c",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-11-09T17:32:16.851691",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_2aaafb8f0bed479192145f68ccc39e5f"
          }
        }
      },
      "execution_count": null,
      "source": "# Load the Community_Safety_Data.csv into a dataframe\ndata = pd.read_csv('Community_Safety_Data.csv')\n\n# Convert the 'year', 'month', and 'day' columns to a datetime object\ndata['Call Date'] = pd.to_datetime(data[['year', 'month', 'day']])\n\n# Display the first few rows to confirm the 'Call Date' column\ndata[['Call Date', 'case_type_pubtrans', 'municipality']].head()",
      "outputs": []
    },
    {
      "id": "24967fab-0c59-496c-b91c-0c9a1a76a869",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "82ba1b5e-5ff9-4952-b584-c3bb27a5d259"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:33:12.688400+00:00",
          "start_time": "2023-11-09T17:33:11.736362+00:00"
        },
        "datalink": {
          "b6cd57cc-f6ce-4c01-bb09-4ab67735a3eb": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": false,
              "orig_num_cols": 2,
              "orig_num_rows": 2,
              "orig_size_bytes": 48,
              "truncated_num_cols": 2,
              "truncated_num_rows": 2,
              "truncated_size_bytes": 48,
              "truncated_string_columns": []
            },
            "display_id": "b6cd57cc-f6ce-4c01-bb09-4ab67735a3eb",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-11-09T17:33:12.272096",
            "user_variable_name": "correlation",
            "variable_name": "correlation"
          }
        }
      },
      "execution_count": null,
      "source": "# Ensure both 'Call Date' and 'Exception Start Time' are of the same data type before merging\ndata['Call Date'] = pd.to_datetime(data['Call Date'])\nstaffing_exceptions_aggregated['Exception Start Time'] = pd.to_datetime(staffing_exceptions_aggregated['Exception Start Time'])\n\n# Merge the calls for service data with the staffing exceptions data on the date\nmerged_data = pd.merge(data, staffing_exceptions_aggregated, left_on='Call Date', right_on='Exception Start Time', how='inner')\n\n# Calculate the total number of calls per day\ncalls_per_day = merged_data.groupby('Call Date').size().reset_index(name='calls_count')\n\n# Merge the calls per day with the exceptions hours per day\ndemand_staffing_correlation = pd.merge(calls_per_day, staffing_exceptions_aggregated, left_on='Call Date', right_on='Exception Start Time', how='inner')\n\n# Calculate the correlation\ncorrelation = demand_staffing_correlation[['calls_count', '# of Hours']].corr()\n\n# Display the correlation matrix\ndisplay(correlation)\n\n# Visualize the correlation with a scatter plot\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=demand_staffing_correlation, x='calls_count', y='# of Hours')\nplt.title('Correlation between Call Demand and Staffing Exceptions Hours')\nplt.xlabel('Total Calls per Day')\nplt.ylabel('Total Staffing Exceptions Hours per Day')\nplt.show()",
      "outputs": []
    },
    {
      "id": "e9b3a3a4-84eb-44c2-9cf2-32f692eb8280",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "f1687ba5-f41b-477f-99c4-911d5a4812d6"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:33:40.817275+00:00",
          "start_time": "2023-11-09T17:33:40.634171+00:00"
        }
      },
      "execution_count": null,
      "source": "# Check for overlapping dates between the two datasets\ncalls_dates = data['Call Date'].dt.date.unique()\nexceptions_dates = staffing_exceptions_aggregated['Exception Start Time'].dt.date.unique()\noverlapping_dates = set(calls_dates).intersection(set(exceptions_dates))\n\n# Display the number of overlapping dates\nlen(overlapping_dates)",
      "outputs": []
    },
    {
      "id": "2e01ea38-8c3b-4ee6-bb91-184561463215",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "87a3c41c-7563-40d0-9fcf-eca53fa19beb"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:37:01.230892+00:00",
          "start_time": "2023-11-09T17:37:00.482892+00:00"
        },
        "datalink": {
          "00b2e13f-d749-4c0f-8f49-a1a42569be4e": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 2,
              "orig_num_rows": 5,
              "orig_size_bytes": 120,
              "truncated_num_cols": 2,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 120,
              "truncated_string_columns": []
            },
            "display_id": "00b2e13f-d749-4c0f-8f49-a1a42569be4e",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-11-09T17:37:01.069928",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_49bc5c9aef0545f9b38b5096d3d8b0be"
          }
        }
      },
      "execution_count": null,
      "source": "# Load the Staffing Exceptions data in chunks due to its large size\nchunk_size = 50000  # Define the chunk size\nchunks = []  # List to hold chunks of dataframes\n\n# Read the file in chunks\nfor chunk in pd.read_csv('District_Platoon_Staffing_Exceptions.csv', chunksize=chunk_size):\n    # Create a 'Date' column by combining 'year', 'month', and 'day'\n    chunk['Date'] = pd.to_datetime(chunk[['year', 'month', 'day']])\n    # Aggregate the data by Date\n    aggregated_chunk = chunk.groupby('Date')['# of Hours'].sum().reset_index(name='exceptions_hours')\n    chunks.append(aggregated_chunk)\n\n# Concatenate chunks into a single dataframe\nstaffing_exceptions_aggregated = pd.concat(chunks, ignore_index=True)\n\n# Display the first few rows of the aggregated exceptions data\ndisplay(staffing_exceptions_aggregated.head())",
      "outputs": []
    },
    {
      "id": "635f2596-d36e-4db5-99ad-a9c9c5881044",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "64ac64c5-4ff3-470a-9982-99b7bae3ec4c"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:36:19.069199+00:00",
          "start_time": "2023-11-09T17:36:18.910079+00:00"
        }
      },
      "execution_count": null,
      "source": "# Load a small sample of the Staffing Exceptions data to check the column names\nsample = pd.read_csv('District_Platoon_Staffing_Exceptions.csv', nrows=5)\n\n# Display the column names\nsample.columns.tolist()",
      "outputs": []
    },
    {
      "id": "4f3c106d-795e-4224-8017-16a9b6898a50",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "f7249d6f-a4ea-4d2f-a470-5294c4baee28"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:37:28.537120+00:00",
          "start_time": "2023-11-09T17:37:27.587227+00:00"
        },
        "datalink": {
          "f7ae503c-bab3-413f-bccb-6178445b9614": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": false,
              "orig_num_cols": 2,
              "orig_num_rows": 2,
              "orig_size_bytes": 48,
              "truncated_num_cols": 2,
              "truncated_num_rows": 2,
              "truncated_size_bytes": 48,
              "truncated_string_columns": []
            },
            "display_id": "f7ae503c-bab3-413f-bccb-6178445b9614",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-11-09T17:37:28.123507",
            "user_variable_name": "correlation",
            "variable_name": "correlation"
          }
        }
      },
      "execution_count": null,
      "source": "# Merge the call demand data with the staffing exceptions data\nmerged_data = pd.merge(calls_per_day, staffing_exceptions_aggregated, left_on='Call Date', right_on='Date', how='inner')\n\n# Calculate the correlation between calls count and exceptions hours\ncorrelation = merged_data[['calls_count', 'exceptions_hours']].corr()\n\n# Display the correlation matrix\ndisplay(correlation)\n\n# Visualize the correlation with a scatter plot\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=merged_data, x='calls_count', y='exceptions_hours')\nplt.title('Correlation between Call Demand and Staffing Exceptions Hours')\nplt.xlabel('Total Calls per Day')\nplt.ylabel('Total Staffing Exceptions Hours per Day')\nplt.show()",
      "outputs": []
    },
    {
      "id": "42352e20-67a4-441c-9d83-ab088ea589f0",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "a3338442-50d4-4f80-bd00-8f6a4d6c030d"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:42:17.292207+00:00",
          "start_time": "2023-11-09T17:42:16.287610+00:00"
        },
        "datalink": {
          "33698cfc-83e8-4433-a2b6-65b52c84335b": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": true,
              "orig_num_cols": 2,
              "orig_num_rows": 5,
              "orig_size_bytes": 120,
              "truncated_num_cols": 2,
              "truncated_num_rows": 5,
              "truncated_size_bytes": 120,
              "truncated_string_columns": []
            },
            "display_id": "33698cfc-83e8-4433-a2b6-65b52c84335b",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-11-09T17:42:17.131342",
            "user_variable_name": null,
            "variable_name": "unk_dataframe_1f38eca2d3a54299a4d41fc8b1bf19d5"
          }
        }
      },
      "execution_count": null,
      "source": "# Load the Calls for Service data in chunks due to its large size\nchunk_size = 50000  # Define the chunk size\nchunks = []  # List to hold chunks of dataframes\n\n# Read the file in chunks\nfor chunk in pd.read_csv('Calls_for_Service.csv', chunksize=chunk_size):\n    # Create a 'Call Date' column by combining 'year', 'month', and 'day'\n    chunk['Call Date'] = pd.to_datetime(chunk[['year', 'month', 'day']])\n    # Aggregate the data by Call Date\n    aggregated_chunk = chunk.groupby('Call Date').size().reset_index(name='calls_count')\n    chunks.append(aggregated_chunk)\n\n# Concatenate chunks into a single dataframe\ncalls_aggregated = pd.concat(chunks, ignore_index=True)\n\n# Display the first few rows of the aggregated calls data\ndisplay(calls_aggregated.head())",
      "outputs": []
    },
    {
      "id": "b5f2e063-6550-452f-a9a3-d7ebb4429ca9",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "46812031-9f4c-47a6-acda-c0c156bab737"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:41:59.073560+00:00",
          "start_time": "2023-11-09T17:41:58.914899+00:00"
        }
      },
      "execution_count": null,
      "source": "# Load a small sample of the Calls for Service data to check the column names\nsample = pd.read_csv('Calls_for_Service.csv', nrows=5)\n\n# Display the column names\nsample.columns.tolist()",
      "outputs": []
    },
    {
      "id": "cc69cb09-5929-447e-9a92-6067b47b1deb",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "844ee322-1a4d-4d3c-9b99-961752c9fcc8"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:42:33.571889+00:00",
          "start_time": "2023-11-09T17:42:32.635945+00:00"
        },
        "datalink": {
          "cd485cd5-7ae9-44c4-8058-30382569e73a": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": false,
              "orig_num_cols": 2,
              "orig_num_rows": 2,
              "orig_size_bytes": 48,
              "truncated_num_cols": 2,
              "truncated_num_rows": 2,
              "truncated_size_bytes": 48,
              "truncated_string_columns": []
            },
            "display_id": "cd485cd5-7ae9-44c4-8058-30382569e73a",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-11-09T17:42:33.143805",
            "user_variable_name": "correlation",
            "variable_name": "correlation"
          }
        }
      },
      "execution_count": null,
      "source": "# Merge the call demand data with the staffing exceptions data\nmerged_data = pd.merge(calls_aggregated, staffing_exceptions_aggregated, left_on='Call Date', right_on='Date', how='inner')\n\n# Calculate the correlation between calls count and exceptions hours\ncorrelation = merged_data[['calls_count', 'exceptions_hours']].corr()\n\n# Display the correlation matrix\ndisplay(correlation)\n\n# Visualize the correlation with a scatter plot\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=merged_data, x='calls_count', y='exceptions_hours')\nplt.title('Correlation between Call Demand and Staffing Exceptions Hours')\nplt.xlabel('Total Calls per Day')\nplt.ylabel('Total Staffing Exceptions Hours per Day')\nplt.show()",
      "outputs": []
    },
    {
      "id": "ddc21691-8f7a-487e-8222-9b9a6c3f1295",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "1a60a22b-4e40-4b90-8a51-db7fef1b7dac"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:55:15.725087+00:00",
          "start_time": "2023-11-09T17:55:15.131922+00:00"
        }
      },
      "execution_count": null,
      "source": "# Prepare the data for time-series forecasting\n# We will use the 'Date' from the staffing exceptions data as the index\nstaffing_data_for_model = staffing_exceptions_aggregated.set_index('Date')\nstaffing_data_for_model.sort_index(inplace=True)\n\n# Check for missing dates and fill them with the previous value (forward-fill)\nstaffing_data_for_model = staffing_data_for_model.asfreq('D', method='ffill')\n\n# Visualize the staffing exceptions hours over time\nplt.figure(figsize=(15, 7))\nplt.plot(staffing_data_for_model.index, staffing_data_for_model['exceptions_hours'], label='Staffing Exceptions Hours')\nplt.title('Staffing Exceptions Hours Over Time')\nplt.xlabel('Date')\nplt.ylabel('Total Staffing Exceptions Hours')\nplt.legend()\nplt.show()\n\n# We will use the ARIMA model for time-series forecasting\n# First, we need to find the optimal parameters for the ARIMA model\nimport pmdarima as pm\n\n# Use auto_arima to find the best parameters\narima_model = pm.auto_arima(staffing_data_for_model['exceptions_hours'], seasonal=True, m=7, suppress_warnings=True)\n\n# Display the summary of the ARIMA model\narima_model.summary()",
      "outputs": []
    },
    {
      "id": "a48afd3b-db05-48c2-8468-917eaaa24e2d",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "73997f0f-bda2-4df9-9990-0ed83409a87b"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:55:27.947983+00:00",
          "start_time": "2023-11-09T17:55:24.768734+00:00"
        }
      },
      "execution_count": null,
      "source": "!pip install -q pmdarima",
      "outputs": []
    },
    {
      "id": "7a2ef486-588c-4093-a1cb-e3ee6792aab8",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "8bb72ef3-1d34-439f-91eb-18eb1637dafb"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T17:55:45.398597+00:00",
          "start_time": "2023-11-09T17:55:45.217876+00:00"
        }
      },
      "execution_count": null,
      "source": "# Now that pmdarima is installed, let's try fitting the ARIMA model again\n\n# Use auto_arima to find the best parameters\narima_model = pm.auto_arima(staffing_data_for_model['exceptions_hours'], seasonal=True, m=7, suppress_warnings=True)\n\n# Display the summary of the ARIMA model\narima_model.summary()",
      "outputs": []
    },
    {
      "id": "e1c22f83-f8aa-4009-9d90-d533ca18f913",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "68414bb0-522f-4c03-95a6-593d664fd935"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T18:03:19.802842+00:00",
          "start_time": "2023-11-09T17:56:06.210888+00:00"
        }
      },
      "execution_count": null,
      "source": "# Import the pmdarima library\nimport pmdarima as pm\n\n# Now that pmdarima is installed, let's try fitting the ARIMA model again\n\n# Use auto_arima to find the best parameters\narima_model = pm.auto_arima(staffing_data_for_model['exceptions_hours'], seasonal=True, m=7, suppress_warnings=True)\n\n# Display the summary of the ARIMA model\narima_model.summary()",
      "outputs": []
    },
    {
      "id": "7981674b-f9c6-4e2a-a861-3135b43ee1c6",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "6b506516-93d0-48cf-ad76-eb351d55ddb3"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T18:03:20.670877+00:00",
          "start_time": "2023-11-09T18:03:19.944506+00:00"
        },
        "datalink": {
          "4e1cd305-fbca-484e-9b1a-e520ac6e2180": {
            "applied_filters": [],
            "dataframe_info": {
              "default_index_used": false,
              "orig_num_cols": 2,
              "orig_num_rows": 2,
              "orig_size_bytes": 48,
              "truncated_num_cols": 2,
              "truncated_num_rows": 2,
              "truncated_size_bytes": 48,
              "truncated_string_columns": []
            },
            "display_id": "4e1cd305-fbca-484e-9b1a-e520ac6e2180",
            "dx_settings": {
              "ALLOW_NOTEABLE_ATTRS": true,
              "COLUMN_SAMPLING_METHOD": "outer",
              "DB_LOCATION": ":memory:",
              "DEV_MODE": false,
              "DISPLAY_MAX_COLUMNS": 100,
              "DISPLAY_MAX_ROWS": 50000,
              "DISPLAY_MODE": "simple",
              "ENABLE_ASSIGNMENT": true,
              "ENABLE_DATALINK": true,
              "FLATTEN_COLUMN_VALUES": true,
              "FLATTEN_INDEX_VALUES": false,
              "GENERATE_DEX_METADATA": false,
              "HTML_TABLE_SCHEMA": false,
              "LOG_LEVEL": 30,
              "MAX_RENDER_SIZE_BYTES": 104857600,
              "MAX_STRING_LENGTH": 250,
              "NUM_PAST_SAMPLES_TRACKED": 3,
              "RANDOM_STATE": 12648430,
              "RESET_INDEX_VALUES": false,
              "ROW_SAMPLING_METHOD": "random",
              "SAMPLING_FACTOR": 0.1,
              "SAMPLING_METHOD": "random",
              "STRINGIFY_COLUMN_VALUES": true,
              "STRINGIFY_INDEX_VALUES": false
            },
            "sample_history": [],
            "sampling_time": "2023-11-09T18:03:20.511317",
            "user_variable_name": "correlation",
            "variable_name": "correlation"
          }
        }
      },
      "execution_count": null,
      "source": "# Merge the call demand data with the staffing exceptions data on the date\nmerged_data = pd.merge(calls_aggregated, staffing_exceptions_aggregated, left_on='Call Date', right_on='Date', how='inner')\n\n# Calculate the correlation between calls count and exceptions hours\ncorrelation = merged_data[['calls_count', 'exceptions_hours']].corr()\n\n# Display the correlation matrix\ndisplay(correlation)",
      "outputs": []
    },
    {
      "id": "6208e15b-1ec2-4076-99c3-b506ba59ae3d",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "da0a22df-0787-467e-ae3c-dbd947708344"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T18:10:09.539764+00:00",
          "start_time": "2023-11-09T18:10:08.467579+00:00"
        }
      },
      "execution_count": null,
      "source": "# For predicting future crime trends and call demands, we will use the calls_aggregated data\n# We will create a time series model using the calls data\ncalls_data_for_model = calls_aggregated.set_index('Call Date')\ncalls_data_for_model.sort_index(inplace=True)\n\n# Check for missing dates and fill them with the previous value (forward-fill)\ncalls_data_for_model = calls_data_for_model.asfreq('D', method='ffill')\n\n# Visualize the calls data over time\nplt.figure(figsize=(15, 7))\nplt.plot(calls_data_for_model.index, calls_data_for_model['calls_count'], label='Calls Count')\nplt.title('Calls Count Over Time')\nplt.xlabel('Date')\nplt.ylabel('Number of Calls')\nplt.legend()\nplt.show()\n\n# We will use the ARIMA model for time-series forecasting\n# First, we need to find the optimal parameters for the ARIMA model\nimport pmdarima as pm\n\n# Use auto_arima to find the best parameters\narima_model_calls = pm.auto_arima(calls_data_for_model['calls_count'], seasonal=True, m=7, suppress_warnings=True)\n\n# Display the summary of the ARIMA model for calls data\narima_model_calls.summary()",
      "outputs": []
    },
    {
      "id": "a3866401-47bc-403c-86b6-51f526fd662e",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "2c6caf78-79ea-4f99-8209-fc8ef29d5e9a"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T18:18:07.408353+00:00",
          "start_time": "2023-11-09T18:10:51.206476+00:00"
        }
      },
      "execution_count": null,
      "source": "# Handle duplicate dates in the calls_aggregated data\ncalls_aggregated_unique = calls_aggregated.groupby('Call Date').sum().reset_index()\n\n# Set the index to 'Call Date' and sort by index\ncalls_data_for_model = calls_aggregated_unique.set_index('Call Date')\ncalls_data_for_model.sort_index(inplace=True)\n\n# Now that we have a unique index, we can reapply the forward-fill method\ncalls_data_for_model = calls_data_for_model.asfreq('D', method='ffill')\n\n# Retry visualizing the calls data over time\nplt.figure(figsize=(15, 7))\nplt.plot(calls_data_for_model.index, calls_data_for_model['calls_count'], label='Calls Count')\nplt.title('Calls Count Over Time')\nplt.xlabel('Date')\nplt.ylabel('Number of Calls')\nplt.legend()\nplt.show()\n\n# Retry fitting the ARIMA model\n# Use auto_arima to find the best parameters\narima_model_calls = pm.auto_arima(calls_data_for_model['calls_count'], seasonal=True, m=7, suppress_warnings=True)\n\n# Display the summary of the ARIMA model for calls data\narima_model_calls.summary()",
      "outputs": []
    },
    {
      "id": "e351be1f-3c44-4f75-b974-9a1db999b674",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "36609ea4-57b8-4028-a092-5149368601ac"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T18:18:07.696761+00:00",
          "start_time": "2023-11-09T18:18:07.415232+00:00"
        }
      },
      "execution_count": null,
      "source": "# Import necessary libraries for machine learning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\n# Prepare the data for the machine learning model\n# For this example, we will use the number of calls as a feature to predict staffing exceptions\nX = merged_data[['calls_count']]  # Features\ny = merged_data['exceptions_hours']  # Target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize the Random Forest Regressor\nrf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n\n# Train the model\nrf_model.fit(X_train, y_train)\n\n# Make predictions\ny_pred = rf_model.predict(X_test)\n\n# Calculate the root mean squared error\nrmse = sqrt(mean_squared_error(y_test, y_pred))\nprint('Root Mean Squared Error:', rmse)",
      "outputs": []
    },
    {
      "id": "d337cabf-0dc2-4d5e-9bb8-70cd8beca66d",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "e554f8d4-2e9e-4636-9107-d857bb1ed1ff"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T18:19:16.122695+00:00",
          "start_time": "2023-11-09T18:19:14.589666+00:00"
        }
      },
      "execution_count": null,
      "source": "# Time-series forecasting for staffing challenges\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n\n# Define the model\nsarimax_model = SARIMAX(staffing_data_for_model['exceptions_hours'],\n                        order=(1, 1, 1),\n                        seasonal_order=(1, 1, 1, 7),\n                        enforce_stationarity=False,\n                        enforce_invertibility=False)\n\n# Fit the model\nsarimax_result = sarimax_model.fit()\n\n# Summary of the model\nsarimax_summary = sarimax_result.summary()\nsarimax_summary",
      "outputs": []
    },
    {
      "id": "29126f1a-e764-4c78-9044-41b91e3176ab",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "00f6cce5-4610-4247-af1c-8fbe70184e48"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T18:28:11.337579+00:00",
          "start_time": "2023-11-09T18:28:10.009007+00:00"
        }
      },
      "execution_count": null,
      "source": "# Diagnostic checking of the SARIMAX model\nsarimax_result.plot_diagnostics(figsize=(15, 12))\nplt.show()\n\n# Forecasting the next 30 days\nforecast = sarimax_result.get_forecast(steps=30)\npredicted_mean = forecast.predicted_mean\nconfidence_intervals = forecast.conf_int()\n\n# Plot the forecast along with the confidence interval\nplt.figure(figsize=(15, 7))\nplt.plot(staffing_data_for_model.index, staffing_data_for_model['exceptions_hours'], label='Observed')\nplt.plot(predicted_mean.index, predicted_mean, label='Forecast', color='r')\nplt.fill_between(predicted_mean.index,\n                 confidence_intervals.iloc[:, 0],\n                 confidence_intervals.iloc[:, 1], color='pink', alpha=0.3)\nplt.title('Staffing Exceptions Hours Forecast')\nplt.xlabel('Date')\nplt.ylabel('Staffing Exceptions Hours')\nplt.legend()\nplt.show()",
      "outputs": []
    },
    {
      "id": "4c517ad0-08df-4917-904c-016d39e2d76a",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code",
          "output_collection_id": "ccd49e0e-e49f-46aa-a9aa-1b3847eab96b"
        },
        "ExecuteTime": {
          "end_time": "2023-11-09T18:35:20.533837+00:00",
          "start_time": "2023-11-09T18:35:20.376231+00:00"
        }
      },
      "execution_count": null,
      "source": "# Scheduling Optimization Recommendations\n# Based on the forecasted staffing challenges, we can provide some recommendations.\n\n# Identify days with the highest forecasted staffing exceptions\nhigh_risk_days = predicted_mean[predicted_mean > predicted_mean.quantile(0.75)]\n\n# Recommendations for scheduling\nrecommendations = []\nfor date, value in high_risk_days.iteritems():\n    recommendations.append(f'On {date.date()}, consider increasing staff or having backup plans due to forecasted high staffing exceptions.')\n\n# Display recommendations\nfor recommendation in recommendations:\n    print(recommendation)",
      "outputs": []
    }
  ]
}